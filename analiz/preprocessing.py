# -*- coding: utf-8 -*-
"""
GeliÅŸmiÅŸ Ã–n Ä°ÅŸleme ModÃ¼lÃ¼
KapsamlÄ± text temizleme, normalizasyon ve Ã¶n iÅŸleme fonksiyonlarÄ±
"""

import re
import string
import unicodedata
from typing import List, Union


# GeniÅŸletilmiÅŸ TÃ¼rkÃ§e stopwords listesi
TURKISH_STOPWORDS = {
    've', 'bir', 'bu', 'da', 'de', 'iÃ§in', 'ile', 'o', 'ki', 'Ã§ok', 'var', 'olan', 'olarak', 'ne',
    'daha', 'kadar', 'en', 'ama', 'ya', 'sadece', 'son', 'ÅŸu', 've', 'tÃ¼m', 'kendi', 'gibi', 'bana',
    'sana', 'ona', 'bize', 'size', 'onlara', 'benim', 'senin', 'onun', 'bizim', 'sizin', 'onlarÄ±n',
    'ben', 'sen', 'biz', 'siz', 'onlar', 'ÅŸey', 'ÅŸeyi', 'ÅŸeyle', 'ÅŸeyin', 'ÅŸeyden', 'ÅŸeyde', 'ÅŸeye',
    'neden', 'nasÄ±l', 'nerede', 'ne', 'kim', 'hangi', 'kaÃ§', 'hem', 'hiÃ§', 'her', 'hep', 'hele',
    'hala', 'hÃ¢lÃ¢', 'henÃ¼z', 'hemen', 'hele', 'hadi', 'haydi', 'iÅŸte', 'artÄ±k', 'zaten', 'yine',
    'gene', 'tekrar', 'yeniden', 'baÅŸka', 'diÄŸer', 'Ã¶bÃ¼r', 'Ã¶teki', 'beriki', 'ÅŸÃ¶yle', 'bÃ¶yle',
    'Ã¶yle', 'aynÄ±', 'benzer', 'farklÄ±', 'baÅŸka', 'birÃ§ok', 'pek', 'oldukÃ§a', 'fazla', 'az', 'biraz',
    'az', 'Ã§ok', 'tam', 'yarÄ±m', 'buÃ§uk', 'epey', 'hayli', 'iyice', 'oldukÃ§a', 'bir', 'iki', 'Ã¼Ã§',
    'ÅŸimdi', 'bugÃ¼n', 'yarÄ±n', 'dÃ¼n', 'Ã¶nceki', 'sonraki', 'gelecek', 'geÃ§en', 'bu', 'ÅŸu', 'o',
    'buraya', 'ÅŸuraya', 'oraya', 'burada', 'ÅŸurada', 'orada', 'buradan', 'ÅŸuradan', 'oradan',
    'yukarÄ±', 'aÅŸaÄŸÄ±', 'ileri', 'geri', 'saÄŸ', 'sol', 'Ã¶n', 'arka', 'iÃ§', 'dÄ±ÅŸ', 'alt', 'Ã¼st',
    'oldu', 'olur', 'oluyor', 'olacak', 'olmuÅŸ', 'olmak', 'var', 'yok', 'vardÄ±', 'yoktu', 'oldu',
    'deÄŸil', 'deÄŸildi', 'deÄŸilmiÅŸ', 'mÄ±', 'mi', 'mu', 'mÃ¼', 'ise', 'imiÅŸ', 'ken', 'iken', 'diye',
    'kez', 'defa', 'sefer', 'gÃ¼n', 'hafta', 'ay', 'yÄ±l', 'asÄ±r', 'Ã§aÄŸ', 'dÃ¶nem', 'zaman', 'vakit',
    'fakat', 'ancak', 'lakin', 'yoksa', 'eÄŸer', 'madem', 'Ã§Ã¼nkÃ¼', 'iÃ§in', 'dolayÄ±', 'sayesinde',
    'raÄŸmen', 'karÅŸÄ±n', 'gÃ¶re', 'dair', 'kadar', 'deÄŸin', 'beri', 'Ã¶nce', 'sonra', 'sÄ±rada',
    'arasÄ±nda', 'iÃ§inde', 'dÄ±ÅŸÄ±nda', 'Ã¼zerinde', 'altÄ±nda', 'yanÄ±nda', 'karÅŸÄ±sÄ±nda', 'arkasÄ±nda',
    'ile', 'den', 'dan', 'ten', 'tan', 'nin', 'nÄ±n', 'nÃ¼n', 'nun', 'ye', 'ya', 'e', 'a',
    'i', 'Ä±', 'u', 'Ã¼', 'o', 'Ã¶', 'de', 'da', 'te', 'ta', 'ne', 'na', 'le', 'la',
    'ler', 'lar', 'dir', 'dÄ±r', 'dur', 'dÃ¼r', 'tir', 'tÄ±r', 'tur', 'tÃ¼r'
}

# URL ve Ã¶zel karakter regex patterns
URL_PATTERN = re.compile(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+')
HTML_TAG_PATTERN = re.compile(r'<[^>]+>')
MENTION_PATTERN = re.compile(r'@\w+')
HASHTAG_PATTERN = re.compile(r'#\w+')
EMAIL_PATTERN = re.compile(r'\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Z|a-z]{2,}\b')
PHONE_PATTERN = re.compile(r'(\+90|0)?[0-9]{3}[\s-]?[0-9]{3}[\s-]?[0-9]{2}[\s-]?[0-9]{2}')
NUMBER_PATTERN = re.compile(r'\b\d+\b')
EMOJI_PATTERN = re.compile(r'[^\w\s\u00C0-\u024F\u1E00-\u1EFF]')
PUNCTUATION_PATTERN = re.compile(r'[^\w\s]')


def remove_urls(text: str) -> str:
    """URL'leri kaldÄ±rÄ±r"""
    return URL_PATTERN.sub('', text)


def remove_html_tags(text: str) -> str:
    """HTML etiketlerini kaldÄ±rÄ±r"""
    return HTML_TAG_PATTERN.sub('', text)


def remove_mentions(text: str) -> str:
    """@mentions kaldÄ±rÄ±r"""
    return MENTION_PATTERN.sub('', text)


def remove_hashtags(text: str, keep_text: bool = False) -> str:
    """Hashtag'leri kaldÄ±rÄ±r veya sadece # iÅŸaretini kaldÄ±rÄ±r"""
    if keep_text:
        return text.replace('#', '')
    return HASHTAG_PATTERN.sub('', text)


def remove_emails(text: str) -> str:
    """E-mail adreslerini kaldÄ±rÄ±r"""
    return EMAIL_PATTERN.sub('', text)


def remove_phone_numbers(text: str) -> str:
    """Telefon numaralarÄ±nÄ± kaldÄ±rÄ±r"""
    return PHONE_PATTERN.sub('', text)


def remove_numbers(text: str) -> str:
    """SayÄ±larÄ± kaldÄ±rÄ±r"""
    return NUMBER_PATTERN.sub('', text)


def remove_emojis(text: str) -> str:
    """Emoji ve Ã¶zel karakterleri kaldÄ±rÄ±r"""
    return EMOJI_PATTERN.sub('', text)


def remove_punctuation(text: str) -> str:
    """Noktalama iÅŸaretlerini kaldÄ±rÄ±r"""
    return PUNCTUATION_PATTERN.sub(' ', text)


def normalize_turkish_chars(text: str) -> str:
    """TÃ¼rkÃ§e karakterleri normalize eder"""
    replacements = {
        'Ä±': 'i', 'Ä°': 'I', 'ÄŸ': 'g', 'Ä': 'G',
        'Ã¼': 'u', 'Ãœ': 'U', 'ÅŸ': 's', 'Å': 'S',
        'Ã¶': 'o', 'Ã–': 'O', 'Ã§': 'c', 'Ã‡': 'C'
    }
    for tr_char, en_char in replacements.items():
        text = text.replace(tr_char, en_char)
    return text


def normalize_unicode(text: str) -> str:
    """Unicode karakterleri normalize eder"""
    return unicodedata.normalize('NFKD', text)


def remove_extra_whitespace(text: str) -> str:
    """Fazla boÅŸluklarÄ± kaldÄ±rÄ±r"""
    return re.sub(r'\s+', ' ', text).strip()


def remove_repeated_chars(text: str, max_repeat: int = 2) -> str:
    """Tekrarlanan karakterleri sÄ±nÄ±rlar (Ã¶rn: 'Ã§ookkkk' -> 'Ã§ook')"""
    pattern = r'(.)\1{' + str(max_repeat) + ',}'
    return re.sub(pattern, r'\1' * max_repeat, text)


def filter_by_length(words: List[str], min_length: int = 2, max_length: int = 50) -> List[str]:
    """Kelime uzunluÄŸuna gÃ¶re filtreler"""
    return [word for word in words if min_length <= len(word) <= max_length]


def remove_stopwords(words: List[str], custom_stopwords: set = None) -> List[str]:
    """Stopword'leri kaldÄ±rÄ±r"""
    stopwords = TURKISH_STOPWORDS.copy()
    if custom_stopwords:
        stopwords.update(custom_stopwords)
    return [word for word in words if word.lower() not in stopwords]


def filter_by_frequency(words: List[str], min_freq: int = 1, max_freq: int = None) -> List[str]:
    """Frekansa gÃ¶re filtreler"""
    from collections import Counter
    word_counts = Counter(words)
    
    if max_freq is None:
        max_freq = len(words)
    
    return [word for word in words 
            if min_freq <= word_counts[word] <= max_freq]


def basic_preprocess(text: str, 
                    lowercase: bool = True,
                    remove_urls_flag: bool = True,
                    remove_html_flag: bool = True,
                    remove_mentions_flag: bool = True,
                    remove_hashtags_flag: bool = True,
                    remove_emails_flag: bool = True,
                    remove_phones_flag: bool = True,
                    remove_numbers_flag: bool = True,
                    remove_emojis_flag: bool = True,
                    remove_punctuation_flag: bool = True,
                    normalize_chars: bool = False,
                    remove_extra_spaces: bool = True) -> str:
    """
    Temel text Ã¶n iÅŸleme fonksiyonu
    """
    if not isinstance(text, str):
        return ""
    
    # Lowercase
    if lowercase:
        text = text.lower()
    
    # URL'leri kaldÄ±r
    if remove_urls_flag:
        text = remove_urls(text)
    
    # HTML etiketlerini kaldÄ±r
    if remove_html_flag:
        text = remove_html_tags(text)
    
    # Mention'larÄ± kaldÄ±r
    if remove_mentions_flag:
        text = remove_mentions(text)
    
    # Hashtag'leri kaldÄ±r
    if remove_hashtags_flag:
        text = remove_hashtags(text)
    
    # E-mail'leri kaldÄ±r
    if remove_emails_flag:
        text = remove_emails(text)
    
    # Telefon numaralarÄ±nÄ± kaldÄ±r
    if remove_phones_flag:
        text = remove_phone_numbers(text)
    
    # SayÄ±larÄ± kaldÄ±r
    if remove_numbers_flag:
        text = remove_numbers(text)
    
    # Emoji'leri kaldÄ±r
    if remove_emojis_flag:
        text = remove_emojis(text)
    
    # Noktalama iÅŸaretlerini kaldÄ±r
    if remove_punctuation_flag:
        text = remove_punctuation(text)
    
    # TÃ¼rkÃ§e karakterleri normalize et
    if normalize_chars:
        text = normalize_turkish_chars(text)
    
    # Unicode normalize
    text = normalize_unicode(text)
    
    # Fazla boÅŸluklarÄ± kaldÄ±r
    if remove_extra_spaces:
        text = remove_extra_whitespace(text)
    
    return text


def advanced_preprocess(text: str,
                       min_word_length: int = 2,
                       max_word_length: int = 50,
                       remove_stopwords_flag: bool = True,
                       custom_stopwords: set = None,
                       min_frequency: int = 1,
                       max_frequency: int = None,
                       max_char_repeat: int = 2) -> List[str]:
    """
    GeliÅŸmiÅŸ text Ã¶n iÅŸleme fonksiyonu
    Text'i Ã¶nce temel Ã¶n iÅŸlemeden geÃ§irir, sonra kelime bazlÄ± iÅŸlemler yapar
    """
    # Ã–nce temel Ã¶n iÅŸleme
    processed_text = basic_preprocess(text)
    
    # Tekrarlanan karakterleri sÄ±nÄ±rla
    if max_char_repeat > 0:
        processed_text = remove_repeated_chars(processed_text, max_char_repeat)
    
    # Kelimelere ayÄ±r
    words = processed_text.split()
    
    # UzunluÄŸa gÃ¶re filtrele
    words = filter_by_length(words, min_word_length, max_word_length)
    
    # Stopword'leri kaldÄ±r
    if remove_stopwords_flag:
        words = remove_stopwords(words, custom_stopwords)
    
    # Frekansa gÃ¶re filtrele
    if min_frequency > 1 or max_frequency is not None:
        words = filter_by_frequency(words, min_frequency, max_frequency)
    
    return words


def preprocess_for_lda(text: str) -> List[str]:
    """LDA analizi iÃ§in Ã¶zel Ã¶n iÅŸleme"""
    return advanced_preprocess(
        text,
        min_word_length=3,
        max_word_length=30,
        remove_stopwords_flag=True,
        min_frequency=2,
        max_char_repeat=2
    )


def preprocess_for_sentiment(text: str) -> str:
    """Sentiment analizi iÃ§in Ã¶zel Ã¶n iÅŸleme"""
    return basic_preprocess(
        text,
        remove_urls_flag=True,
        remove_html_flag=True,
        remove_mentions_flag=True,
        remove_hashtags_flag=False,  # Hashtag'ler sentiment iÃ§in Ã¶nemli olabilir
        remove_emails_flag=True,
        remove_phones_flag=True,
        remove_numbers_flag=False,   # SayÄ±lar sentiment iÃ§in Ã¶nemli olabilir
        remove_emojis_flag=False,    # Emoji'ler sentiment iÃ§in Ã¶nemli
        remove_punctuation_flag=False,  # Noktalama sentiment iÃ§in Ã¶nemli
        normalize_chars=False
    )


def preprocess_for_wordcloud(text: str) -> List[str]:
    """Word cloud iÃ§in Ã¶zel Ã¶n iÅŸleme"""
    return advanced_preprocess(
        text,
        min_word_length=3,
        max_word_length=25,
        remove_stopwords_flag=True,
        min_frequency=2,
        max_char_repeat=2
    )


def batch_preprocess(texts: List[str], 
                    preprocess_func,
                    show_progress: bool = False) -> List[Union[str, List[str]]]:
    """
    Toplu text Ã¶n iÅŸleme fonksiyonu
    """
    results = []
    total = len(texts)
    
    for i, text in enumerate(texts):
        if show_progress and i % 100 == 0:
            print(f"Ä°ÅŸlenen: {i}/{total} ({i/total*100:.1f}%)")
        
        processed = preprocess_func(text)
        results.append(processed)
    
    if show_progress:
        print(f"TamamlandÄ±: {total}/{total} (100.0%)")
    
    return results


# Backward compatibility iÃ§in eski fonksiyon isimleri
def clean_text(text: str) -> str:
    """Geriye uyumluluk iÃ§in - basic_preprocess ile aynÄ±"""
    return basic_preprocess(text)


def tokenize_and_clean(text: str) -> List[str]:
    """Geriye uyumluluk iÃ§in - advanced_preprocess ile aynÄ±"""
    return advanced_preprocess(text)


if __name__ == "__main__":
    # Test Ã¶rneÄŸi
    sample_text = """
    Merhaba! Bu bir test metnidir... ğŸ“± http://example.com #test @kullanici
    Bu metinde Ã§oook fazla tekrarlanan harfler var!!! 
    Email: test@example.com Tel: 0555 123 45 67
    TÃ¼rkÃ§e karakterler: Ã§ÄŸÄ±Ã¶ÅŸÃ¼ 123456
    """
    
    print("Orijinal text:")
    print(sample_text)
    print("\n" + "="*50 + "\n")
    
    print("Temel Ã¶n iÅŸleme:")
    print(basic_preprocess(sample_text))
    print("\n" + "="*50 + "\n")
    
    print("GeliÅŸmiÅŸ Ã¶n iÅŸleme:")
    print(advanced_preprocess(sample_text))
    print("\n" + "="*50 + "\n")
    
    print("LDA iÃ§in Ã¶n iÅŸleme:")
    print(preprocess_for_lda(sample_text))
    print("\n" + "="*50 + "\n")
    
    print("Sentiment iÃ§in Ã¶n iÅŸleme:")
    print(preprocess_for_sentiment(sample_text))
    print("\n" + "="*50 + "\n")
    
    print("WordCloud iÃ§in Ã¶n iÅŸleme:")
    print(preprocess_for_wordcloud(sample_text)) 