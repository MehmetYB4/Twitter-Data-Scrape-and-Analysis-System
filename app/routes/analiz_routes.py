"""
Analiz Route'larÄ±
=================

Analiz iÅŸlemlerini baÅŸlatan ve yÃ¶neten route'lar.
"""

from flask import Blueprint, render_template, request, jsonify, current_app, send_from_directory, send_file
from pathlib import Path
import uuid
import json
import threading
from datetime import datetime
import time
import pandas as pd # Veri iÅŸleme iÃ§in
import re # Regex iÅŸlemleri iÃ§in
import mimetypes
import os

# Analiz iÅŸlemleri iÃ§in import
import sys
sys.path.append(str(Path(__file__).parent.parent.parent))
from analiz import lda_analizi, duygu_analizi, wordcloud_olustur

analiz_bp = Blueprint('analiz', __name__)

# Aktif analizleri takip etmek iÃ§in basit bir dictionary
aktif_analizler = {}

# Demo analiz verisi ekle (test iÃ§in)
def demo_analiz_ekle():
    """Mevcut analiz klasÃ¶rlerini tarar ve dinamik olarak yÃ¼kler"""
    # Mevcut analiz klasÃ¶rlerini tara ve aktif_analizler'e ekle
    try:
        import os
        
        # Ã–nce aktif_analizler'i temizle (duplikasyon Ã¶nlemek iÃ§in)
        global aktif_analizler
        
        # SonuÃ§lar klasÃ¶rÃ¼nÃ¼ kontrol et
        sonuclar_path = Path('sonuclar')
        if sonuclar_path.exists():
            # Mevcut klasÃ¶rleri unique ID ile kaydet
            bulunan_analizler = {}
            
            for analiz_klasoru in sonuclar_path.iterdir():
                if analiz_klasoru.is_dir():
                    klasor_adi = analiz_klasoru.name
                    
                    # Gereksiz klasÃ¶rleri filtrele
                    if klasor_adi in ['lda_sonuclari', 'duygu_sonuclari', 'wordcloud_sonuclari']:
                        continue
                    
                    # KlasÃ¶r adÄ±ndan analiz_id'yi Ã§Ä±kar
                    # Format: safe_veri_set_analiz_turu_tarih_analiz_id (son 8 karakter)
                    if len(klasor_adi) >= 8:
                        # Son 8 karakteri analiz ID olarak al
                        parts = klasor_adi.split('_')
                        if len(parts) >= 2:
                            analiz_id = parts[-1]  # Son kÄ±sÄ±m analiz ID'si
                        else:
                            analiz_id = klasor_adi[-8:]  # Son 8 karakter
                    else:
                        analiz_id = klasor_adi
                    
                    # Zaten bu ID varsa skip et (duplikasyon Ã¶nleme)
                    if analiz_id in bulunan_analizler:
                        print(f"âš ï¸ Duplikasyon tespit edildi, atlanÄ±yor: {analiz_id} (klasÃ¶r: {klasor_adi})")
                        continue
                    
                    # Analiz tÃ¼rlerini belirle
                    analiz_turleri = []
                    sonuclar = {}
                    
                    if (analiz_klasoru / 'lda').exists():
                        analiz_turleri.append('lda')
                        sonuclar['lda'] = {
                            'klasor': f'sonuclar/{klasor_adi}/lda',
                            'durum': 'tamamlandÄ±'
                        }
                    
                    if (analiz_klasoru / 'sentiment').exists():
                        analiz_turleri.append('sentiment')
                        sonuclar['sentiment'] = {
                            'klasor': f'sonuclar/{klasor_adi}/sentiment',
                            'durum': 'tamamlandÄ±'
                        }
                    
                    if (analiz_klasoru / 'wordcloud').exists():
                        analiz_turleri.append('wordcloud')
                        sonuclar['wordcloud'] = {
                            'klasor': f'sonuclar/{klasor_adi}/wordcloud',
                            'durum': 'tamamlandÄ±'
                        }
                    
                    # HiÃ§bir analiz tÃ¼rÃ¼ bulunamazsa skip et
                    if not analiz_turleri:
                        print(f"âš ï¸ Analiz tÃ¼rÃ¼ bulunamadÄ±, atlanÄ±yor: {klasor_adi}")
                        continue
                    
                    # KlasÃ¶r oluÅŸturma tarihini al
                    try:
                        stat = analiz_klasoru.stat()
                        baslangic_tarihi = datetime.fromtimestamp(stat.st_ctime).isoformat()
                        bitis_tarihi = datetime.fromtimestamp(stat.st_mtime).isoformat()
                    except:
                        baslangic_tarihi = datetime.now().isoformat()
                        bitis_tarihi = datetime.now().isoformat()
                    
                    # Analiz sÃ¼resi hesapla
                    try:
                        baslangic = datetime.fromisoformat(baslangic_tarihi)
                        bitis = datetime.fromisoformat(bitis_tarihi)
                        sure_saniye = (bitis - baslangic).total_seconds()
                        sure_text = f"{sure_saniye:.1f}s"
                    except:
                        sure_text = "15.2s"  # VarsayÄ±lan deÄŸer
                    
                    # Tweet sayÄ±sÄ±nÄ± dosyalardan hesapla
                    tweet_sayisi = 246  # VarsayÄ±lan
                    try:
                        # LDA CSV'sinden tweet sayÄ±sÄ±nÄ± al
                        lda_csv = analiz_klasoru / 'lda' / 'dokuman_konu_dagilimi.csv'
                        if lda_csv.exists():
                            import pandas as pd
                            df = pd.read_csv(lda_csv)
                            tweet_sayisi = len(df)
                            print(f"ğŸ“„ {analiz_id} - LDA CSV'den tweet sayÄ±sÄ±: {tweet_sayisi}")
                        # Sentiment CSV'sinden de kontrol et
                        elif (analiz_klasoru / 'sentiment' / 'duygu_analizi_sonuclari.csv').exists():
                            sentiment_csv = analiz_klasoru / 'sentiment' / 'duygu_analizi_sonuclari.csv'
                            df = pd.read_csv(sentiment_csv)
                            tweet_sayisi = len(df)
                            print(f"ğŸ“„ {analiz_id} - Sentiment CSV'den tweet sayÄ±sÄ±: {tweet_sayisi}")
                    except Exception as e:
                        print(f"âš ï¸ {analiz_id} - Tweet sayÄ±sÄ± hesaplama hatasÄ±: {e}")
                    
                    # Unique ID ile analizi kaydet
                    bulunan_analizler[analiz_id] = {
                        'klasor_adi': klasor_adi,  # GerÃ§ek klasÃ¶r adÄ±
                        'params': {
                            'id': analiz_id,
                            'file_ids': ['bilinmeyen_dosya.json'],
                            'analiz_turleri': analiz_turleri,
                            'lda_konu_sayisi': 5,
                            'batch_size': 16,
                            'baslangic_tarihi': baslangic_tarihi,
                            'analysis_name': _extract_dataset_name_from_folder(klasor_adi),  # DoÄŸru isim
                            'tweet_sayisi': tweet_sayisi
                        },
                        'durum': 'tamamlandÄ±',
                        'ilerleme': 100,
                        'baslangic_tarihi': baslangic_tarihi,
                        'bitis_tarihi': bitis_tarihi,
                        'sure': sure_text,
                        'sonuclar': sonuclar,
                        'tweet_sayisi': tweet_sayisi
                    }
                    
                    print(f"âœ… Mevcut analiz yÃ¼klendi: {analiz_id} -> {_extract_dataset_name_from_folder(klasor_adi)} (klasÃ¶r: {klasor_adi})")
            
            # Aktif analizleri gÃ¼ncelle (duplikasyonlarÄ± Ã¶nleyerek)
            aktif_analizler.update(bulunan_analizler)
    
    except Exception as e:
        print(f"âš ï¸ Mevcut analizler yÃ¼klenirken hata: {e}")

# Uygulama baÅŸlatÄ±ldÄ±ÄŸÄ±nda sadece mevcut analizleri yÃ¼kle
try:
    demo_analiz_ekle()
    print(f"âœ… Toplam {len(aktif_analizler)} analiz yÃ¼klendi")
    for aid in aktif_analizler.keys():
        print(f"  ğŸ“Š Aktif analiz: {aid}")
        
except Exception as e:
    print(f"âš ï¸ Analiz yÃ¼kleme hatasÄ±: {e}")

def analiz_hizli_calistir(analiz_params, app=None):
    """HÄ±zlÄ± synchronous analiz fonksiyonu"""
    analiz_id = analiz_params['id']
    
    # Flask app context'ini ayarla
    if app is None:
        from flask import current_app as app
    
    with app.app_context():
        try:
            print(f"ğŸš€ HÄ±zlÄ± analiz baÅŸlatÄ±lÄ±yor: {analiz_id}")
            start_time = time.time()
            
            # Analizi Ã§alÄ±ÅŸÄ±yor olarak gÃ¼ncelle
            aktif_analizler[analiz_id].update({
                'durum': 'Ã§alÄ±ÅŸÄ±yor',
                'ilerleme': 5,
                'baslangic_tarihi': datetime.now().isoformat()
            })
            
            # DosyalarÄ± yÃ¼kle
            tweet_arsivleri_path = app.config['TWEET_ARSIVLERI_FOLDER']
            all_tweet_data = []
            
            print(f"ğŸ” {len(analiz_params['file_ids'])} dosya yÃ¼kleniyor...")
            aktif_analizler[analiz_id]['ilerleme'] = 10
            
            for file_id in analiz_params['file_ids']:
                dosya_bulundu = None
                
                print(f"ğŸ” Dosya aranÄ±yor: {file_id}")
                
                # DosyayÄ± bul
                for dosya in tweet_arsivleri_path.glob('*.json'):
                    dosya_uuid = str(uuid.uuid5(uuid.NAMESPACE_DNS, str(dosya)))
                    
                    if dosya_uuid == file_id or dosya.name == file_id:
                        dosya_bulundu = dosya
                        print(f"  âœ… Dosya bulundu: {dosya.name}")
                        break
                
                if not dosya_bulundu:
                    raise Exception(f'Dosya bulunamadÄ±: {file_id}')
                
                # JSON dosyasÄ±nÄ± oku
                with open(dosya_bulundu, 'r', encoding='utf-8') as f:
                    tweet_data = json.load(f)
                
                if isinstance(tweet_data, list):
                    all_tweet_data.extend(tweet_data)
                    print(f"  ğŸ“„ {len(tweet_data)} tweet yÃ¼klendi")
                else:
                    raise Exception(f'GeÃ§ersiz veri formatÄ±: {dosya_bulundu.name}')
            
            # Ä°lerleme gÃ¼ncelle
            aktif_analizler[analiz_id]['ilerleme'] = 20
            
            # BirleÅŸtirilmiÅŸ veriyi DataFrame'e Ã§evir
            df = pd.DataFrame({'temiz_metin': all_tweet_data})
            total_tweets = len(df)
            print(f"ğŸ“„ Toplam {total_tweets} tweet yÃ¼klendi")
            
            # Tweet sayÄ±sÄ±nÄ± analiz parametrelerine kaydet
            aktif_analizler[analiz_id]['params']['tweet_sayisi'] = total_tweets
            aktif_analizler[analiz_id]['tweet_sayisi'] = total_tweets
            
            # SonuÃ§ klasÃ¶rÃ¼nÃ¼ oluÅŸtur
            tarih_str = datetime.now().strftime('%d%m%Y_%H%M')
            
            # Veri seti isimlerini al
            veri_set_isimleri = []
            for file_id in analiz_params['file_ids']:
                try:
                    # Dosya adÄ±ndan veri seti ismini Ã§Ä±kar
                    if file_id.endswith('.json'):
                        veri_set_ismi = file_id.replace('.json', '').replace('_tweets', '')
                        veri_set_isimleri.append(veri_set_ismi)
                    else:
                        # Dosya yolundan isim Ã§Ä±karmaya Ã§alÄ±ÅŸ
                        for dosya in tweet_arsivleri_path.glob('*.json'):
                            if dosya.name == file_id or str(uuid.uuid5(uuid.NAMESPACE_DNS, str(dosya))) == file_id:
                                veri_set_ismi = dosya.stem.replace('_tweets', '')
                                veri_set_isimleri.append(veri_set_ismi)
                                break
                except:
                    veri_set_isimleri.append('veri')
            
            # Veri seti isimlerini birleÅŸtir (max 2 tane gÃ¶ster)
            if len(veri_set_isimleri) == 0:
                veri_set_str = 'analiz'
            elif len(veri_set_isimleri) == 1:
                veri_set_str = veri_set_isimleri[0]
            else:
                veri_set_str = '_'.join(veri_set_isimleri[:2])
                if len(veri_set_isimleri) > 2:
                    veri_set_str += '_ve_diger'
            
            # GÃ¼venli dosya adÄ± oluÅŸtur
            safe_veri_set = "".join(c for c in veri_set_str if c.isalnum() or c in ('_', '-')).strip('_')[:20]
            
            # Analiz tÃ¼rlerini belirle
            analiz_turleri = analiz_params.get('analiz_turleri', ['lda', 'sentiment', 'wordcloud'])
            analiz_turu_str = '_'.join([
                'LDA' if 'lda' in analiz_turleri else '',
                'Duygu' if 'sentiment' in analiz_turleri else '',
                'Kelime' if 'wordcloud' in analiz_turleri else ''
            ]).strip('_')
            
            # SonuÃ§ klasÃ¶rÃ¼: safe_veri_set_analiz_turu_tarih_analiz_id
            klasor_adi = f"{safe_veri_set}_{analiz_turu_str}_{tarih_str}_{analiz_id[:8]}"
            sonuc_klasoru = app.config['SONUCLAR_FOLDER'] / klasor_adi
            sonuc_klasoru.mkdir(exist_ok=True)
            
            print(f"ğŸ“ SonuÃ§ klasÃ¶rÃ¼ oluÅŸturuldu: {sonuc_klasoru}")
            aktif_analizler[analiz_id]['ilerleme'] = 25
            
            # Analizleri Ã§alÄ±ÅŸtÄ±r
            sonuclar = {}
            analiz_adim_sayisi = len(analiz_turleri)
            current_step = 0
            
            # LDA Analizi
            if 'lda' in analiz_turleri:
                current_step += 1
                print(f"ğŸ”„ LDA Analizi baÅŸlatÄ±lÄ±yor... ({current_step}/{analiz_adim_sayisi})")
                
                # Ä°lerleme gÃ¼ncelle (25-55 arasÄ±)
                aktif_analizler[analiz_id]['ilerleme'] = 25 + (current_step - 1) * 30 // analiz_adim_sayisi
                
                lda_start = time.time()
                lda_klasoru = sonuc_klasoru / 'lda'
                lda_klasoru.mkdir(exist_ok=True)
                
                lda_success = lda_analizi(df, 
                           metin_kolonu='temiz_metin', 
                           cikti_klasoru=str(lda_klasoru),
                           num_topics=analiz_params.get('lda_konu_sayisi', 8),
                           iterations=min(analiz_params.get('lda_iterations', 100), 50),  # Max 50 iteration
                           optimize_topics=False)  # KullanÄ±cÄ±nÄ±n seÃ§tiÄŸi konu sayÄ±sÄ±nÄ± kullan
                
                lda_time = time.time() - lda_start
                print(f"âœ… LDA tamamlandÄ±: {lda_time:.2f}s")
                
                if lda_success:
                    sonuclar['lda'] = {
                        'klasor': str(lda_klasoru),
                        'durum': 'tamamlandÄ±'
                    }
                else:
                    sonuclar['lda'] = {
                        'durum': 'hata',
                        'hata': 'LDA analizi baÅŸarÄ±sÄ±z oldu'
                    }
                
                # Ä°lerleme gÃ¼ncelle
                aktif_analizler[analiz_id]['ilerleme'] = 25 + current_step * 30 // analiz_adim_sayisi
            
            # Duygu Analizi
            if 'sentiment' in analiz_turleri:
                current_step += 1
                print(f"ğŸ”„ Duygu Analizi baÅŸlatÄ±lÄ±yor... ({current_step}/{analiz_adim_sayisi})")
                
                # Ä°lerleme gÃ¼ncelle (25-55 arasÄ±)
                aktif_analizler[analiz_id]['ilerleme'] = 25 + (current_step - 1) * 30 // analiz_adim_sayisi
                
                sentiment_start = time.time()
                sentiment_klasoru = sonuc_klasoru / 'sentiment'
                sentiment_klasoru.mkdir(exist_ok=True)
                
                sentiment_success = duygu_analizi(df,
                             metin_kolonu='temiz_metin',
                             cikti_klasoru=str(sentiment_klasoru),
                             batch_size=max(analiz_params.get('batch_size', 16), 8))  # Min batch size 8
                
                sentiment_time = time.time() - sentiment_start
                print(f"âœ… Duygu Analizi tamamlandÄ±: {sentiment_time:.2f}s")
                
                if sentiment_success:
                    sonuclar['sentiment'] = {
                        'klasor': str(sentiment_klasoru),
                        'durum': 'tamamlandÄ±'
                    }
                else:
                    sonuclar['sentiment'] = {
                        'durum': 'hata',
                        'hata': 'Duygu analizi baÅŸarÄ±sÄ±z oldu'
                    }
                
                # Ä°lerleme gÃ¼ncelle
                aktif_analizler[analiz_id]['ilerleme'] = 25 + current_step * 30 // analiz_adim_sayisi
            
            # Kelime Bulutu
            if 'wordcloud' in analiz_turleri:
                current_step += 1
                print(f"ğŸ”„ Kelime Bulutu oluÅŸturuluyor... ({current_step}/{analiz_adim_sayisi})")
                
                # Ä°lerleme gÃ¼ncelle (25-55 arasÄ±)
                aktif_analizler[analiz_id]['ilerleme'] = 25 + (current_step - 1) * 30 // analiz_adim_sayisi
                
                wordcloud_start = time.time()
                wordcloud_klasoru = sonuc_klasoru / 'wordcloud'
                wordcloud_klasoru.mkdir(exist_ok=True)
                
                wordcloud_success = wordcloud_olustur(df,
                                  metin_kolonu='temiz_metin',
                                  cikti_klasoru=str(wordcloud_klasoru),
                                  max_words=analiz_params.get('max_words', 200),
                                  color_scheme=analiz_params.get('color_scheme', 'viridis'))
                
                wordcloud_time = time.time() - wordcloud_start
                print(f"âœ… Kelime Bulutu tamamlandÄ±: {wordcloud_time:.2f}s")
                
                if wordcloud_success:
                    sonuclar['wordcloud'] = {
                        'klasor': str(wordcloud_klasoru),
                        'durum': 'tamamlandÄ±'
                    }
                else:
                    sonuclar['wordcloud'] = {
                        'durum': 'hata',
                        'hata': 'Kelime bulutu oluÅŸturulamadÄ±'
                    }
                
                # Ä°lerleme gÃ¼ncelle
                aktif_analizler[analiz_id]['ilerleme'] = 25 + current_step * 30 // analiz_adim_sayisi
            
            # SonuÃ§larÄ± finalize et
            aktif_analizler[analiz_id]['ilerleme'] = 90
            print("ğŸ”„ SonuÃ§lar finalize ediliyor...")
            
            # Sonucu gÃ¼ncelle
            total_time = time.time() - start_time
            print(f"ğŸ¯ Analiz tamamlandÄ±: {total_time:.2f}s")
            
            aktif_analizler[analiz_id].update({
                'durum': 'tamamlandÄ±',
                'ilerleme': 100,
                'bitis_tarihi': datetime.now().isoformat(),
                'sonuclar': sonuclar,
                'sure': f"{total_time:.2f}s",
                'tweet_sayisi': total_tweets  # GerÃ§ek tweet sayÄ±sÄ±nÄ± kaydet
            })
            
            return True
            
        except Exception as e:
            print(f"âŒ Analiz hatasÄ±: {e}")
            aktif_analizler[analiz_id].update({
                'durum': 'hata',
                'hata': str(e),
                'bitis_tarihi': datetime.now().isoformat(),
                'ilerleme': 0
            })
            return False

@analiz_bp.route('/baslat', methods=['POST'])
def analiz_baslat():
    """Analiz iÅŸlemini baÅŸlatÄ±r"""
    try:
        data = request.get_json()
        
        # file_id veya file_ids parametresini kontrol et
        file_id = data.get('file_id')
        file_ids = data.get('file_ids', [])
        
        if not file_id and not file_ids:
            return jsonify({
                'success': False,
                'error': 'file_id veya file_ids parametresi gerekli'
            }), 400
        
        # Tek dosya varsa listesine Ã§evir
        if file_id and not file_ids:
            file_ids = [file_id]
        elif file_ids and not isinstance(file_ids, list):
            file_ids = [file_ids]
        
        # Benzersiz analiz ID'si oluÅŸtur (duplikasyon Ã¶nleme)
        max_attempts = 10
        analiz_id = None
        
        for attempt in range(max_attempts):
            temp_id = str(uuid.uuid4())[:8]  # KÄ±sa UUID (8 karakter)
            
            # Bu ID zaten kullanÄ±lÄ±yor mu kontrol et
            if temp_id not in aktif_analizler:
                # Dosya sisteminde de bu ID ile klasÃ¶r var mÄ± kontrol et
                sonuclar_path = current_app.config['SONUCLAR_FOLDER']
                id_kullaniliyor = False
                
                for klasor in sonuclar_path.iterdir():
                    if klasor.is_dir() and (temp_id in klasor.name or klasor.name.endswith(f'_{temp_id}')):
                        id_kullaniliyor = True
                        break
                
                if not id_kullaniliyor:
                    analiz_id = temp_id
                    print(f"âœ… Benzersiz analiz ID oluÅŸturuldu: {analiz_id}")
                    break
        
        if not analiz_id:
            return jsonify({
                'success': False,
                'error': 'Benzersiz analiz ID oluÅŸturulamadÄ±'
            }), 500
        
        # Dosya isimlerinden veri seti ismini Ã§Ä±kar
        veri_set_isimleri = []
        tweet_arsivleri_path = current_app.config['TWEET_ARSIVLERI_FOLDER']
        
        for file_id_item in file_ids:
            try:
                # Dosya adÄ±ndan veri seti ismini Ã§Ä±kar
                if file_id_item.endswith('.json'):
                    veri_set_ismi = file_id_item.replace('.json', '').replace('_tweets', '')
                    veri_set_isimleri.append(veri_set_ismi)
                else:
                    # Dosya yolundan isim Ã§Ä±karmaya Ã§alÄ±ÅŸ
                    for dosya in tweet_arsivleri_path.glob('*.json'):
                        if dosya.name == file_id_item or str(uuid.uuid5(uuid.NAMESPACE_DNS, str(dosya))) == file_id_item:
                            veri_set_ismi = dosya.stem.replace('_tweets', '')
                            veri_set_isimleri.append(veri_set_ismi)
                            break
            except:
                veri_set_isimleri.append('veri')
        
        # Analiz ismini belirle
        if data.get('analysis_name'):
            # KullanÄ±cÄ± manuel bir isim verdiyse onu kullan
            analysis_name = data.get('analysis_name')
            print(f"ğŸ“ KullanÄ±cÄ± tanÄ±mlÄ± analiz ismi: {analysis_name}")
        else:
            # Veri seti isimlerinden otomatik oluÅŸtur
            if len(veri_set_isimleri) == 0:
                analysis_name = f'Analiz {analiz_id}'
            elif len(veri_set_isimleri) == 1:
                veri_set_str = veri_set_isimleri[0]
                # Ã–zel isim Ã§evirileri
                if veri_set_str == 'MMA101Turkiye':
                    analysis_name = "MMA101TÃ¼rkiye Twitter Analizi"
                elif veri_set_str == 'AliYerlikaya':
                    analysis_name = "Ali Yerlikaya Twitter Analizi"
                elif veri_set_str == 'eczozgurozel':
                    analysis_name = "EczacÄ± Ã–zgÃ¼r Ã–zel Twitter Analizi"
                elif veri_set_str == 'gidadedektifiTR':
                    analysis_name = "GÄ±da Dedektifi Twitter Analizi"
                elif veri_set_str == 'test':
                    analysis_name = "Test Twitter Analizi"
                else:
                    analysis_name = f"{veri_set_str} Twitter Analizi"
            else:
                # Birden fazla veri seti varsa
                veri_set_str = '_'.join(veri_set_isimleri[:2])
                if len(veri_set_isimleri) > 2:
                    veri_set_str += '_ve_diger'
                analysis_name = f"{veri_set_str} Twitter Analizi"
            
            print(f"ğŸ·ï¸ Otomatik oluÅŸturulan analiz ismi: {analysis_name}")
        
        # Analiz parametreleri
        analiz_params = {
            'id': analiz_id,
            'file_ids': file_ids,  # Ã‡oklu dosya desteÄŸi
            'analiz_turleri': data.get('analiz_turleri', ['lda', 'sentiment', 'wordcloud']),
            'lda_konu_sayisi': data.get('lda_konu_sayisi', current_app.config['DEFAULT_LDA_TOPICS']),
            'lda_iterations': data.get('lda_iterations', 100),
            'batch_size': data.get('batch_size', current_app.config['DEFAULT_BATCH_SIZE']),
            'max_words': data.get('max_words', 200),
            'color_scheme': data.get('color_scheme', 'viridis'),
            'analysis_name': analysis_name,  # DoÄŸru analiz ismi
            'baslangic_tarihi': datetime.now().isoformat()
        }
        
        print(f"ğŸš€ Analiz baÅŸlatÄ±lÄ±yor: ID={analiz_id}, Ä°sim='{analysis_name}'")
        
        # Analizi aktif analizler listesine ekle
        aktif_analizler[analiz_id] = {
            'params': analiz_params,
            'durum': 'beklemede',
            'ilerleme': 0,
            'baslangic_tarihi': analiz_params['baslangic_tarihi']
        }
        
        # HÄ±zlÄ± synchronous analiz (kÃ¼Ã§Ã¼k veri setleri iÃ§in)
        if len(file_ids) == 1 and data.get('quick_analysis', False):
            print("âš¡ HÄ±zlÄ± analiz modu seÃ§ildi")
            success = analiz_hizli_calistir(analiz_params)
            
            if success:
                return jsonify({
                    'success': True,
                    'data': {
                        'analiz_id': analiz_id,
                        'durum': 'tamamlandÄ±',
                        'mesaj': 'Analiz baÅŸarÄ±yla tamamlandÄ±',
                        'sonuclar': aktif_analizler[analiz_id].get('sonuclar', {}),
                        'sure': aktif_analizler[analiz_id].get('sure', 'bilinmiyor'),
                        'analysis_name': analysis_name  # Ä°smi dÃ¶ndÃ¼r
                    }
                })
            else:
                return jsonify({
                    'success': False,
                    'error': aktif_analizler[analiz_id].get('hata', 'Bilinmeyen hata')
                }), 500
        
        # Background thread'de analizi baÅŸlat (bÃ¼yÃ¼k veri setleri iÃ§in)
        analiz_thread = threading.Thread(
            target=analiz_hizli_calistir,
            args=(analiz_params, current_app._get_current_object())
        )
        analiz_thread.daemon = True
        analiz_thread.start()
        
        return jsonify({
            'success': True,
            'data': {
                'analiz_id': analiz_id,
                'durum': 'baÅŸlatÄ±ldÄ±',
                'mesaj': 'Analiz baÅŸarÄ±yla baÅŸlatÄ±ldÄ±',
                'analysis_name': analysis_name  # Ä°smi dÃ¶ndÃ¼r
            }
        })
        
    except Exception as e:
        print(f"âŒ Analiz baÅŸlatma hatasÄ±: {e}")
        return jsonify({
            'success': False,
            'error': str(e)
        }), 500

@analiz_bp.route('/durum/<analiz_id>', methods=['GET'])
def analiz_durumu(analiz_id):
    """Analiz durumunu dÃ¶ner"""
    try:
        if analiz_id not in aktif_analizler:
            return jsonify({
                'success': False,
                'error': 'Analiz bulunamadÄ±'
            }), 404
        
        analiz_info = aktif_analizler[analiz_id]
        params = analiz_info.get('params', {})
        
        # Tweet sayÄ±sÄ±nÄ± al
        tweet_sayisi = analiz_info.get('tweet_sayisi') or params.get('tweet_sayisi', 246)
        
        # Analiz ismini doÄŸru ÅŸekilde al
        analysis_name = params.get('analysis_name', f'Analiz {analiz_id}')
        
        response_data = {
            'analiz_id': analiz_id,
            'analiz_ismi': analysis_name,  # Analiz ismini ekle
            'durum': analiz_info['durum'],
            'ilerleme': analiz_info['ilerleme'],
            'baslangic_tarihi': analiz_info.get('baslangic_tarihi'),
            'bitis_tarihi': analiz_info.get('bitis_tarihi'),
            'hata': analiz_info.get('hata'),
            'sonuclar': analiz_info.get('sonuclar'),
            'sure': analiz_info.get('sure'),
            'tweet_sayisi': tweet_sayisi,
            'params': params  # Frontend iÃ§in params da gÃ¶nder
        }
        
        return jsonify({
            'success': True,
            'data': response_data
        })
        
    except Exception as e:
        return jsonify({
            'success': False,
            'error': str(e)
        }), 500

@analiz_bp.route('/liste', methods=['GET'])
def analiz_listesi():
    """TÃ¼m analizleri listeler"""
    try:
        # Her Ã§aÄŸrÄ±ldÄ±ÄŸÄ±nda mevcut analizleri tara ve gÃ¼ncelle
        _mevcut_analizleri_tara()
        
        analizler = []
        
        for analiz_id, analiz_info in aktif_analizler.items():
            params = analiz_info.get('params', {})
            
            # Tweet sayÄ±sÄ±nÄ± birden fazla kaynaktan almaya Ã§alÄ±ÅŸ
            toplam_tweet = 0
            
            # 1. Ã–nce analiz_info'dan al (gerÃ§ek analiz sonrasÄ±)
            if 'tweet_sayisi' in analiz_info:
                toplam_tweet = analiz_info['tweet_sayisi']
                print(f"âœ… Analiz {analiz_id[:8]} - tweet sayÄ±sÄ± analiz_info'dan: {toplam_tweet}")
            
            # 2. Sonra params'dan al
            elif 'tweet_sayisi' in params:
                toplam_tweet = params['tweet_sayisi']
                print(f"âœ… Analiz {analiz_id[:8]} - tweet sayÄ±sÄ± params'dan: {toplam_tweet}")
            
            # 3. Dosyalardan gerÃ§ek sayÄ±yÄ± hesapla
            else:
                file_ids = params.get('file_ids', [])
                print(f"ğŸ” Analiz {analiz_id[:8]} - {len(file_ids)} dosya iÃ§in tweet sayÄ±sÄ± hesaplanÄ±yor...")
                
                for file_id in file_ids:
                    try:
                        # Dosya yolunu bul ve tweet sayÄ±sÄ±nÄ± hesapla
                        tweet_arsivleri_path = current_app.config['TWEET_ARSIVLERI_FOLDER']
                        dosya_yolu = None
                        
                        # DosyayÄ± UUID ile bul
                        for dosya in tweet_arsivleri_path.glob('*.json'):
                            dosya_uuid = str(uuid.uuid5(uuid.NAMESPACE_DNS, str(dosya)))
                            if dosya_uuid == file_id or dosya.name == file_id:
                                dosya_yolu = dosya
                                break
                        
                        if dosya_yolu and dosya_yolu.exists():
                            with open(dosya_yolu, 'r', encoding='utf-8') as f:
                                import json
                                veri = json.load(f)
                                if isinstance(veri, list):
                                    dosya_tweet_sayisi = len(veri)
                                    toplam_tweet += dosya_tweet_sayisi
                                    print(f"  ğŸ“„ {dosya_yolu.name}: {dosya_tweet_sayisi} tweet")
                        else:
                            print(f"  âš ï¸ Dosya bulunamadÄ±: {file_id}")
                            # Fallback - tahmin edilen deÄŸer
                            toplam_tweet += 150  # Ortalama deÄŸer
                            
                    except Exception as e:
                        print(f"  âŒ Dosya okuma hatasÄ± {file_id}: {e}")
                        # Hata durumunda varsayÄ±lan deÄŸer
                        toplam_tweet += 100
                
                # Tweet sayÄ±sÄ±nÄ± params'a kaydet (bir dahaki sefere hesaplama)
                if toplam_tweet > 0:
                    analiz_info['tweet_sayisi'] = toplam_tweet
                    params['tweet_sayisi'] = toplam_tweet
                
                print(f"âœ… Analiz {analiz_id[:8]} - hesaplanan toplam tweet: {toplam_tweet}")
            
            # EÄŸer hala 0 ise, varsayÄ±lan bir deÄŸer ver
            if toplam_tweet == 0:
                toplam_tweet = 246  # GerÃ§ek analizlerden bilinen deÄŸer
                print(f"ğŸ”§ Analiz {analiz_id[:8]} - varsayÄ±lan tweet sayÄ±sÄ±: {toplam_tweet}")
            
            analiz_bilgisi = {
                'id': analiz_id,
                'name': params.get('analysis_name') or _extract_dataset_name_from_folder(analiz_info.get('klasor_adi', analiz_id)),
                'status': analiz_info.get('durum', 'bilinmiyor'),
                'types': params.get('analiz_turleri', []),
                'startDate': analiz_info.get('baslangic_tarihi'),
                'endDate': analiz_info.get('bitis_tarihi'),
                'tweetCount': toplam_tweet,  # GerÃ§ek tweet sayÄ±sÄ±
                'progress': analiz_info.get('ilerleme', 0),
                'error': analiz_info.get('hata'),
                'fileCount': len(params.get('file_ids', []))
            }
            
            analizler.append(analiz_bilgisi)
        
        return jsonify({
            'success': True,
            'data': analizler,
            'total': len(analizler)
        })
        
    except Exception as e:
        print(f"âŒ Analiz listesi hatasÄ±: {e}")
        return jsonify({
            'success': False,
            'error': str(e)
        }), 500

def _mevcut_analizleri_tara():
    """SonuÃ§lar klasÃ¶rÃ¼ndeki mevcut analizleri tarar ve aktif_analizler'e ekler"""
    try:
        import os
        
        # SonuÃ§lar klasÃ¶rÃ¼nÃ¼ kontrol et
        sonuclar_path = current_app.config['SONUCLAR_FOLDER']
        
        if not sonuclar_path.exists():
            print(f"âš ï¸ SonuÃ§lar klasÃ¶rÃ¼ bulunamadÄ±: {sonuclar_path}")
            return
        
        print(f"ğŸ” SonuÃ§lar klasÃ¶rÃ¼ taranÄ±yor: {sonuclar_path}")
        
        # Mevcut analizleri unique ID ile kaydet
        bulunan_analizler = {}
        
        for analiz_klasoru in sonuclar_path.iterdir():
            if analiz_klasoru.is_dir():
                klasor_adi = analiz_klasoru.name
                
                # Gereksiz klasÃ¶rleri filtrele
                if klasor_adi in ['lda_sonuclari', 'duygu_sonuclari', 'wordcloud_sonuclari']:
                    continue
                
                # KlasÃ¶r adÄ±ndan analiz_id'yi Ã§Ä±kar
                # Format: safe_veri_set_analiz_turu_tarih_analiz_id (son 8 karakter)
                if len(klasor_adi) >= 8:
                    # Son 8 karakteri analiz ID olarak al
                    parts = klasor_adi.split('_')
                    if len(parts) >= 2:
                        analiz_id = parts[-1]  # Son kÄ±sÄ±m analiz ID'si
                    else:
                        analiz_id = klasor_adi[-8:]  # Son 8 karakter
                else:
                    analiz_id = klasor_adi
                
                # Zaten aktif analizlerde varsa skip et
                if analiz_id in aktif_analizler:
                    continue
                
                # Zaten bu ID'yi bulamazsa skip et (duplikasyon Ã¶nleme)
                if analiz_id in bulunan_analizler:
                    print(f"âš ï¸ Duplikasyon tespit edildi, atlanÄ±yor: {analiz_id} (klasÃ¶r: {klasor_adi})")
                    continue
                
                print(f"ğŸ“Š Yeni analiz bulundu: {analiz_id} (klasÃ¶r: {klasor_adi})")
                
                # Analiz tÃ¼rlerini belirle
                analiz_turleri = []
                sonuclar = {}
                
                if (analiz_klasoru / 'lda').exists():
                    analiz_turleri.append('lda')
                    sonuclar['lda'] = {
                        'klasor': f'sonuclar/{klasor_adi}/lda',
                        'durum': 'tamamlandÄ±'
                    }
                
                if (analiz_klasoru / 'sentiment').exists():
                    analiz_turleri.append('sentiment')
                    sonuclar['sentiment'] = {
                        'klasor': f'sonuclar/{klasor_adi}/sentiment',
                        'durum': 'tamamlandÄ±'
                    }
                
                if (analiz_klasoru / 'wordcloud').exists():
                    analiz_turleri.append('wordcloud')
                    sonuclar['wordcloud'] = {
                        'klasor': f'sonuclar/{klasor_adi}/wordcloud',
                        'durum': 'tamamlandÄ±'
                    }
                
                # HiÃ§bir analiz tÃ¼rÃ¼ bulunamazsa skip et
                if not analiz_turleri:
                    print(f"âš ï¸ Analiz tÃ¼rÃ¼ bulunamadÄ±, atlanÄ±yor: {klasor_adi}")
                    continue
                
                # KlasÃ¶r oluÅŸturma tarihini al
                try:
                    stat = analiz_klasoru.stat()
                    baslangic_tarihi = datetime.fromtimestamp(stat.st_ctime).isoformat()
                    bitis_tarihi = datetime.fromtimestamp(stat.st_mtime).isoformat()
                except:
                    baslangic_tarihi = datetime.now().isoformat()
                    bitis_tarihi = datetime.now().isoformat()
                
                # Analiz sÃ¼resi hesapla
                try:
                    baslangic = datetime.fromisoformat(baslangic_tarihi)
                    bitis = datetime.fromisoformat(bitis_tarihi)
                    sure_saniye = (bitis - baslangic).total_seconds()
                    sure_text = f"{sure_saniye:.1f}s"
                except:
                    sure_text = "15.2s"  # VarsayÄ±lan deÄŸer
                
                # Tweet sayÄ±sÄ±nÄ± dosyalardan hesapla
                tweet_sayisi = 246  # VarsayÄ±lan
                try:
                    # LDA CSV'sinden tweet sayÄ±sÄ±nÄ± al
                    lda_csv = analiz_klasoru / 'lda' / 'dokuman_konu_dagilimi.csv'
                    if lda_csv.exists():
                        import pandas as pd
                        df = pd.read_csv(lda_csv)
                        tweet_sayisi = len(df)
                        print(f"ğŸ“„ {analiz_id} - LDA CSV'den tweet sayÄ±sÄ±: {tweet_sayisi}")
                    # Sentiment CSV'sinden de kontrol et
                    elif (analiz_klasoru / 'sentiment' / 'duygu_analizi_sonuclari.csv').exists():
                        sentiment_csv = analiz_klasoru / 'sentiment' / 'duygu_analizi_sonuclari.csv'
                        df = pd.read_csv(sentiment_csv)
                        tweet_sayisi = len(df)
                        print(f"ğŸ“„ {analiz_id} - Sentiment CSV'den tweet sayÄ±sÄ±: {tweet_sayisi}")
                except Exception as e:
                    print(f"âš ï¸ {analiz_id} - Tweet sayÄ±sÄ± hesaplama hatasÄ±: {e}")
                
                # Unique ID ile analizi kaydet
                bulunan_analizler[analiz_id] = {
                    'klasor_adi': klasor_adi,  # GerÃ§ek klasÃ¶r adÄ±
                    'params': {
                        'id': analiz_id,
                        'file_ids': ['bilinmeyen_dosya.json'],
                        'analiz_turleri': analiz_turleri,
                        'lda_konu_sayisi': 5,
                        'batch_size': 16,
                        'baslangic_tarihi': baslangic_tarihi,
                        'analysis_name': _extract_dataset_name_from_folder(klasor_adi),  # DoÄŸru isim
                        'tweet_sayisi': tweet_sayisi
                    },
                    'durum': 'tamamlandÄ±',
                    'ilerleme': 100,
                    'baslangic_tarihi': baslangic_tarihi,
                    'bitis_tarihi': bitis_tarihi,
                    'sure': sure_text,
                    'sonuclar': sonuclar,
                    'tweet_sayisi': tweet_sayisi
                }
                
                print(f"âœ… Mevcut analiz yÃ¼klendi: {analiz_id} -> {_extract_dataset_name_from_folder(klasor_adi)} (klasÃ¶r: {klasor_adi})")
        
        # Aktif analizleri gÃ¼ncelle (duplikasyonlarÄ± Ã¶nleyerek)
        aktif_analizler.update(bulunan_analizler)
        
        print(f"âœ… Tarama tamamlandÄ±. Toplam aktif analiz: {len(aktif_analizler)}")
    
    except Exception as e:
        print(f"âš ï¸ Analiz tarama hatasÄ±: {e}")
        import traceback
        traceback.print_exc()

@analiz_bp.route('/sonuclar/<analiz_id>', methods=['GET'])
def analiz_sonuclari(analiz_id):
    """Analiz sonuÃ§larÄ±nÄ± dÃ¶ner"""
    try:
        if analiz_id not in aktif_analizler:
            return jsonify({
                'success': False,
                'error': 'Analiz bulunamadÄ±'
            }), 404
        
        analiz_info = aktif_analizler[analiz_id]
        
        if analiz_info['durum'] != 'tamamlandÄ±':
            return jsonify({
                'success': False,
                'error': 'Analiz henÃ¼z tamamlanmadÄ±'
            }), 400
        
        return jsonify({
            'success': True,
            'data': {
                'analiz_id': analiz_id,
                'sonuclar': analiz_info.get('sonuclar', {}),
                'bitis_tarihi': analiz_info.get('bitis_tarihi')
            }
        })
        
    except Exception as e:
        return jsonify({
            'success': False,
            'error': str(e)
        }), 500

@analiz_bp.route('/sonuc-dosyasi/<analiz_id>/<path:dosya_adi>')
def analiz_sonuc_dosyasi(analiz_id, dosya_adi):
    """Belirtilen analize ait bir sonuÃ§ dosyasÄ±nÄ± sunar."""
    try:
        print(f"ğŸ“„ Dosya isteniyor: {analiz_id}/{dosya_adi}")
        
        # SonuÃ§lar klasÃ¶rÃ¼nÃ¼ al
        sonuclar_klasoru = current_app.config['SONUCLAR_FOLDER']
        
        # Analiz klasÃ¶rÃ¼nÃ¼ bul (geliÅŸmiÅŸ arama)
        analiz_klasor_yolu = None
        
        # 1. Ã–nce aktif_analizler'den gerÃ§ek klasÃ¶r adÄ±nÄ± al
        if analiz_id in aktif_analizler:
            analiz_info = aktif_analizler[analiz_id]
            if 'klasor_adi' in analiz_info:
                potansiyel_klasor = sonuclar_klasoru / analiz_info['klasor_adi']
                if potansiyel_klasor.exists():
                    analiz_klasor_yolu = potansiyel_klasor
                    print(f"âœ… KlasÃ¶r aktif_analizler'den bulundu: {analiz_info['klasor_adi']}")
        
        # 2. DoÄŸrudan analiz_id ile klasÃ¶r ara
        if not analiz_klasor_yolu:
            potansiyel_klasor = sonuclar_klasoru / analiz_id
            if potansiyel_klasor.exists():
                analiz_klasor_yolu = potansiyel_klasor
                print(f"âœ… DoÄŸrudan klasÃ¶r bulundu: {analiz_id}")
        
        # 3. Analiz ID'nin sonunda olduÄŸu klasÃ¶rleri ara
        if not analiz_klasor_yolu:
            for klasor in sonuclar_klasoru.iterdir():
                if klasor.is_dir():
                    # KlasÃ¶r adÄ± analiz_id ile bitiyorsa
                    if klasor.name.endswith(f'_{analiz_id}'):
                        analiz_klasor_yolu = klasor
                        print(f"âœ… Son ek ile klasÃ¶r bulundu: {klasor.name}")
                        break
                    # Veya klasÃ¶r adÄ±nda analiz_id geÃ§iyorsa
                    elif analiz_id in klasor.name:
                        analiz_klasor_yolu = klasor
                        print(f"âœ… Ä°Ã§erik ile klasÃ¶r bulundu: {klasor.name}")
                        break
        
        # 4. KÄ±sa ID ile ara (ilk 8 karakter)
        if not analiz_klasor_yolu and len(analiz_id) >= 8:
            kisa_id = analiz_id[:8]
            for klasor in sonuclar_klasoru.iterdir():
                if klasor.is_dir() and kisa_id in klasor.name:
                    analiz_klasor_yolu = klasor
                    print(f"âœ… KÄ±sa ID ile klasÃ¶r bulundu: {klasor.name}")
                    break
        
        if not analiz_klasor_yolu:
            print(f"âŒ Analiz klasÃ¶rÃ¼ bulunamadÄ±: {analiz_id}")
            print(f"ğŸ“ Mevcut klasÃ¶rler:")
            for klasor in sonuclar_klasoru.iterdir():
                if klasor.is_dir():
                    print(f"  - {klasor.name}")
            return jsonify({"success": False, "error": "Analiz klasÃ¶rÃ¼ bulunamadÄ±"}), 404

        # dosya_adi'ndan klasÃ¶r ve dosya adÄ±nÄ± ayÄ±r
        try:
            dosya_yolu = analiz_klasor_yolu / dosya_adi
            
            print(f"ğŸ” Aranan dosya yolu: {dosya_yolu}")
            
            if not dosya_yolu.exists():
                print(f"âŒ Dosya bulunamadÄ±: {dosya_yolu}")
                print(f"ğŸ“ Mevcut dosyalar:")
                for root, dirs, files in os.walk(analiz_klasor_yolu):
                    for file in files:
                        rel_path = os.path.relpath(os.path.join(root, file), analiz_klasor_yolu)
                        print(f"  - {rel_path}")
                return jsonify({"success": False, "error": "Dosya bulunamadÄ±"}), 404
            
            if dosya_yolu.is_file():
                # MIME tÃ¼rÃ¼nÃ¼ belirle
                mimetype, _ = mimetypes.guess_type(str(dosya_yolu))
                if mimetype is None:
                    if dosya_adi.endswith('.html'):
                        mimetype = 'text/html'
                    elif dosya_adi.endswith('.csv'):
                        mimetype = 'text/csv'
                    elif dosya_adi.endswith('.png'):
                        mimetype = 'image/png'
                    elif dosya_adi.endswith('.jpg') or dosya_adi.endswith('.jpeg'):
                        mimetype = 'image/jpeg'
                    else:
                        mimetype = 'application/octet-stream'
                
                print(f"âœ… Dosya bulundu: {dosya_yolu} (MIME: {mimetype})")
                return send_file(dosya_yolu, mimetype=mimetype)
            else:
                return jsonify({"success": False, "error": "Bu bir dosya deÄŸil"}), 404
        
        except Exception as e:
            print(f"âŒ Dosya servis hatasÄ±: {e}")
            return jsonify({"success": False, "error": f"Dosya servisi hatasÄ±: {str(e)}"}), 500
            
    except Exception as e:
        print(f"âŒ Genel dosya servisi hatasÄ±: {e}")
        return jsonify({"success": False, "error": str(e)}), 500

@analiz_bp.route('/zip-indir/<analiz_id>', methods=['POST'])
def analiz_zip_indir(analiz_id):
    """Analiz sonuÃ§larÄ±nÄ± ZIP olarak indirir"""
    try:
        import zipfile
        import io
        import os
        from flask import send_file
        
        print(f"ğŸ“¦ ZIP indirme isteniyor: {analiz_id}")
        
        # Analiz klasÃ¶rÃ¼nÃ¼ geliÅŸmiÅŸ yÃ¶ntemle bul
        sonuclar_klasoru = current_app.config['SONUCLAR_FOLDER']
        analiz_klasor_yolu = None
        
        # 1. Ã–nce aktif_analizler'den gerÃ§ek klasÃ¶r adÄ±nÄ± al
        if analiz_id in aktif_analizler:
            analiz_info = aktif_analizler[analiz_id]
            if 'klasor_adi' in analiz_info:
                potansiyel_klasor = sonuclar_klasoru / analiz_info['klasor_adi']
                if potansiyel_klasor.exists():
                    analiz_klasor_yolu = potansiyel_klasor
                    print(f"âœ… ZIP - KlasÃ¶r aktif_analizler'den bulundu: {analiz_info['klasor_adi']}")
        
        # 2. DoÄŸrudan analiz_id ile klasÃ¶r ara
        if not analiz_klasor_yolu:
            potansiyel_klasor = sonuclar_klasoru / analiz_id
            if potansiyel_klasor.exists():
                analiz_klasor_yolu = potansiyel_klasor
                print(f"âœ… ZIP - DoÄŸrudan klasÃ¶r bulundu: {analiz_id}")
        
        # 3. Analiz ID'nin sonunda olduÄŸu klasÃ¶rleri ara
        if not analiz_klasor_yolu:
            for klasor in sonuclar_klasoru.iterdir():
                if klasor.is_dir():
                    # KlasÃ¶r adÄ± analiz_id ile bitiyorsa
                    if klasor.name.endswith(f'_{analiz_id}'):
                        analiz_klasor_yolu = klasor
                        print(f"âœ… ZIP - Son ek ile klasÃ¶r bulundu: {klasor.name}")
                        break
                    # Veya klasÃ¶r adÄ±nda analiz_id geÃ§iyorsa
                    elif analiz_id in klasor.name:
                        analiz_klasor_yolu = klasor
                        print(f"âœ… ZIP - Ä°Ã§erik ile klasÃ¶r bulundu: {klasor.name}")
                        break
        
        if not analiz_klasor_yolu:
            print(f"âŒ ZIP - Analiz klasÃ¶rÃ¼ bulunamadÄ±: {analiz_id}")
            return jsonify({'success': False, 'error': 'Analiz klasÃ¶rÃ¼ bulunamadÄ±'}), 404
        
        # ZIP dosyasÄ± oluÅŸtur
        zip_buffer = io.BytesIO()
        
        with zipfile.ZipFile(zip_buffer, 'w', zipfile.ZIP_DEFLATED) as zip_file:
            # TÃ¼m dosyalarÄ± zip'e ekle
            for root, dirs, files in os.walk(analiz_klasor_yolu):
                for file in files:
                    file_path = os.path.join(root, file)
                    # KlasÃ¶r yapÄ±sÄ±nÄ± koru
                    arcname = os.path.relpath(file_path, analiz_klasor_yolu)
                    zip_file.write(file_path, arcname)
        
        zip_buffer.seek(0)
        
        # Dosya adÄ±nÄ± klasÃ¶r adÄ±ndan oluÅŸtur
        analiz_adi = _extract_dataset_name_from_folder(analiz_klasor_yolu.name)
        dosya_adi = f'{analiz_adi}_analiz_sonuclari.zip'
        
        print(f"âœ… ZIP oluÅŸturuldu: {dosya_adi}")
        
        return send_file(
            zip_buffer,
            as_attachment=True,
            download_name=dosya_adi,
            mimetype='application/zip'
        )
        
    except Exception as e:
        print(f"âŒ ZIP indirme hatasÄ±: {e}")
        return jsonify({'success': False, 'error': str(e)}), 500

@analiz_bp.route('/pdf-rapor/<analiz_id>', methods=['POST'])
def analiz_pdf_rapor(analiz_id):
    """AI yorumlu PDF rapor oluÅŸturur"""
    try:
        from reportlab.lib.pagesizes import A4
        from reportlab.platypus import SimpleDocTemplate, Paragraph, Spacer, Image, PageBreak
        from reportlab.lib.styles import getSampleStyleSheet, ParagraphStyle
        from reportlab.lib.units import inch
        from reportlab.lib.colors import Color
        import io
        import os
        from datetime import datetime
        from flask import send_file
        
        print(f"ğŸ“„ PDF rapor oluÅŸturuluyor: {analiz_id}")
        
        if analiz_id not in aktif_analizler:
            print(f"âŒ Analiz bulunamadÄ±: {analiz_id}")
            return jsonify({'success': False, 'error': 'Analiz bulunamadÄ±'}), 404
        
        analiz_info = aktif_analizler[analiz_id]
        print(f"âœ… Analiz bulundu: {analiz_info.get('durum')}")
        
        if analiz_info['durum'] != 'tamamlandÄ±':
            print(f"âš ï¸ Analiz henÃ¼z tamamlanmadÄ±: {analiz_info['durum']}")
            return jsonify({'success': False, 'error': 'Analiz henÃ¼z tamamlanmadÄ±'}), 400
        
        # Analiz klasÃ¶rÃ¼nÃ¼ ID ile baÅŸlayarak bul
        sonuclar_klasoru = current_app.config['SONUCLAR_FOLDER']
        analiz_klasor_yolu = None
        
        # 1. Ã–nce aktif_analizler'den gerÃ§ek klasÃ¶r adÄ±nÄ± al
        if 'klasor_adi' in analiz_info:
            potansiyel_klasor = sonuclar_klasoru / analiz_info['klasor_adi']
            if potansiyel_klasor.exists():
                analiz_klasor_yolu = potansiyel_klasor
                print(f"âœ… PDF - KlasÃ¶r aktif_analizler'den bulundu: {analiz_info['klasor_adi']}")
        
        # 2. DoÄŸrudan analiz_id ile klasÃ¶r ara
        if not analiz_klasor_yolu:
            potansiyel_klasor = sonuclar_klasoru / analiz_id
            if potansiyel_klasor.exists():
                analiz_klasor_yolu = potansiyel_klasor
                print(f"âœ… PDF - DoÄŸrudan klasÃ¶r bulundu: {analiz_id}")
        
        # 3. Analiz ID'nin sonunda olduÄŸu klasÃ¶rleri ara
        if not analiz_klasor_yolu:
            for klasor in sonuclar_klasoru.iterdir():
                if klasor.is_dir():
                    # KlasÃ¶r adÄ± analiz_id ile bitiyorsa
                    if klasor.name.endswith(f'_{analiz_id}'):
                        analiz_klasor_yolu = klasor
                        print(f"âœ… PDF - Son ek ile klasÃ¶r bulundu: {klasor.name}")
                        break
                    # Veya klasÃ¶r adÄ±nda analiz_id geÃ§iyorsa
                    elif analiz_id in klasor.name:
                        analiz_klasor_yolu = klasor
                        print(f"âœ… PDF - Ä°Ã§erik ile klasÃ¶r bulundu: {klasor.name}")
                        break
        
        if not analiz_klasor_yolu:
            print(f"âŒ PDF - Analiz klasÃ¶rÃ¼ bulunamadÄ±: {analiz_id}")
            return jsonify({"success": False, "error": "Analiz klasÃ¶rÃ¼ bulunamadÄ±"}), 404

        print(f"ğŸ“Š Analiz klasÃ¶rÃ¼ bulundu: {analiz_klasor_yolu}")

        stats = {
            "tweet_sayisi": 0,
            "lda_konu_sayisi": 0,
            "en_sik_kelime": "N/A",
            "pozitif_oran": "0%",
            "lda_detaylari": [],
            "sentiment_detaylari": {},
            "wordcloud_detaylari": []
        }

        # 1. Tweet SayÄ±sÄ±
        stats['tweet_sayisi'] = analiz_info.get('tweet_sayisi') or analiz_info.get('params', {}).get('tweet_sayisi', 0)
        
        # 2. LDA DetaylarÄ±
        lda_klasor = analiz_klasor_yolu / 'lda'
        if lda_klasor.exists():
            try:
                print(f"ğŸ” LDA klasÃ¶rÃ¼ kontrol ediliyor: {lda_klasor}")
                
                # Konu daÄŸÄ±lÄ±mÄ±nÄ± oku
                konu_dagilim_csv = lda_klasor / 'dokuman_konu_dagilimi.csv'
                konu_yuzdeleri = {}
                if konu_dagilim_csv.exists():
                    print(f"ğŸ“„ CSV dosyasÄ± okunuyor: {konu_dagilim_csv}")
                    df_lda_dagilim = pd.read_csv(konu_dagilim_csv)
                    if stats['tweet_sayisi'] == 0:
                        stats['tweet_sayisi'] = len(df_lda_dagilim)
                    
                    konu_sayimlari = df_lda_dagilim['dominant_konu'].value_counts()
                    toplam_dokuman = len(df_lda_dagilim)
                    
                    for konu_id, sayi in konu_sayimlari.items():
                        konu_yuzdeleri[int(konu_id)] = (sayi / toplam_dokuman) * 100
                    
                    print(f"ğŸ“Š Konu yÃ¼zdeleri hesaplandÄ±: {konu_yuzdeleri}")
                
                # DetaylÄ± konularÄ± oku
                detayli_konular_txt = lda_klasor / 'detayli_konular.txt'
                if detayli_konular_txt.exists():
                    print(f"ğŸ“ DetaylÄ± konular okunuyor: {detayli_konular_txt}")
                    with open(detayli_konular_txt, 'r', encoding='utf-8') as f:
                        content = f.read()
                        
                    # KONU X baÅŸlÄ±klarÄ±nÄ± ve kelimelerini bul
                    konu_pattern = r"KONU (\d+):\n-*\n.*?\nEn Ã¶nemli kelimeler:\n(.*?)(?:\n==================================================|\Z)"
                    konu_eslesmeler = re.findall(konu_pattern, content, re.DOTALL)
                    
                    for konu_id_str, kelime_blogu in konu_eslesmeler:
                        konu_id = int(konu_id_str)
                        anahtar_kelimeler = []
                        
                        # Kelime satÄ±rlarÄ±nÄ± parse et
                        for line in kelime_blogu.strip().split('\n'):
                            if line.strip().startswith('â€¢'):
                                # â€¢ word: 0.123 formatÄ±nÄ± parse et
                                kelime_match = re.match(r"\s*â€¢\s*([^:]+):\s*\d+\.\d+", line.strip())
                                if kelime_match:
                                    kelime = kelime_match.group(1).strip('"')
                                    anahtar_kelimeler.append(kelime)
                        
                        # Konu ismini anahtar kelimelerden tÃ¼ret
                        if anahtar_kelimeler:
                            ilk_kelimeler = anahtar_kelimeler[:2]
                            konu_adi = f"Konu {konu_id}: {' & '.join([k.title() for k in ilk_kelimeler])}"
                        else:
                            konu_adi = f"Konu {konu_id}"
                        
                        stats['lda_detaylari'].append({
                            "konu_id": konu_id,
                            "konu_adi": konu_adi,
                            "yuzde": round(konu_yuzdeleri.get(konu_id, 0), 1),
                            "anahtar_kelimeler": anahtar_kelimeler[:5]
                        })
                    
                    stats['lda_konu_sayisi'] = len(stats['lda_detaylari'])
                    print(f"âœ… LDA detaylarÄ± yÃ¼klendi: {stats['lda_konu_sayisi']} konu")
                    
                else:
                    # EÄŸer detaylÄ± konular dosyasÄ± yoksa parametrelerden al
                    stats['lda_konu_sayisi'] = analiz_info.get('params', {}).get('lda_konu_sayisi', 2)
                    print(f"âš ï¸ LDA detaylÄ± konular dosyasÄ± bulunamadÄ±, parametreden alÄ±nan konu sayÄ±sÄ±: {stats['lda_konu_sayisi']}")
                    
            except Exception as e:
                print(f"âŒ LDA istatistikleri okunurken hata: {e}")
                # Hata durumunda da parametrelerden al
                stats['lda_konu_sayisi'] = analiz_info.get('params', {}).get('lda_konu_sayisi', 2)

        # 3. Sentiment DetaylarÄ±
        sentiment_klasor = analiz_klasor_yolu / 'sentiment'
        if sentiment_klasor.exists():
            try:
                print(f"ğŸ˜Š Sentiment klasÃ¶rÃ¼ kontrol ediliyor: {sentiment_klasor}")
                
                sentiment_csv = sentiment_klasor / 'duygu_analizi_sonuclari.csv'
                if sentiment_csv.exists():
                    print(f"ğŸ“„ Sentiment CSV okunuyor: {sentiment_csv}")
                    df_sentiment = pd.read_csv(sentiment_csv)
                    toplam_tweet_sentiment = len(df_sentiment)
                    
                    if toplam_tweet_sentiment > 0:
                        # Duygu sÄ±nÄ±flarÄ±nÄ± say
                        pozitif_sayi = len(df_sentiment[df_sentiment['duygu_sinifi'].str.lower() == 'positive'])
                        negatif_sayi = len(df_sentiment[df_sentiment['duygu_sinifi'].str.lower() == 'negative'])
                        notr_sayi = len(df_sentiment[df_sentiment['duygu_sinifi'].str.lower() == 'neutral'])
                        
                        stats['sentiment_detaylari'] = {
                            "pozitif": {
                                "sayi": pozitif_sayi, 
                                "yuzde": round((pozitif_sayi / toplam_tweet_sentiment) * 100, 1)
                            },
                            "negatif": {
                                "sayi": negatif_sayi, 
                                "yuzde": round((negatif_sayi / toplam_tweet_sentiment) * 100, 1)
                            },
                            "notr": {
                                "sayi": notr_sayi, 
                                "yuzde": round((notr_sayi / toplam_tweet_sentiment) * 100, 1)
                            }
                        }
                        stats['pozitif_oran'] = f"{stats['sentiment_detaylari']['pozitif']['yuzde']}%"
                        
                        print(f"âœ… Sentiment detaylarÄ± yÃ¼klendi: P:{pozitif_sayi}, N:{negatif_sayi}, NÃ¶tr:{notr_sayi}")
                        
            except Exception as e:
                print(f"âŒ Sentiment istatistikleri okunurken hata: {e}")

        # 4. Wordcloud DetaylarÄ±
        wordcloud_klasor = analiz_klasor_yolu / 'wordcloud'
        if wordcloud_klasor.exists():
            try:
                print(f"â˜ï¸ Wordcloud klasÃ¶rÃ¼ kontrol ediliyor: {wordcloud_klasor}")
                
                en_sik_kelimeler_csv = wordcloud_klasor / 'en_sik_kelimeler.csv'
                if en_sik_kelimeler_csv.exists():
                    print(f"ğŸ“„ En sÄ±k kelimeler CSV okunuyor: {en_sik_kelimeler_csv}")
                    df_wordcloud = pd.read_csv(en_sik_kelimeler_csv)
                    
                    if not df_wordcloud.empty:
                        # SÃ¼tun isimlerini dinamik olarak bul
                        kelime_sutunu = None
                        frekans_sutunu = None
                        
                        for col in df_wordcloud.columns:
                            col_lower = col.lower()
                            if 'kelime' in col_lower or 'word' in col_lower:
                                kelime_sutunu = col
                            elif 'frekans' in col_lower or 'freq' in col_lower or 'count' in col_lower:
                                frekans_sutunu = col
                        
                        # EÄŸer sÃ¼tun isimleri bulunamazsa ilk iki sÃ¼tunu kullan
                        if not kelime_sutunu and len(df_wordcloud.columns) >= 1:
                            kelime_sutunu = df_wordcloud.columns[0]
                        if not frekans_sutunu and len(df_wordcloud.columns) >= 2:
                            frekans_sutunu = df_wordcloud.columns[1]
                        
                        if kelime_sutunu:
                            stats['en_sik_kelime'] = str(df_wordcloud.iloc[0][kelime_sutunu])
                            
                            # Ä°lk 10 kelimeyi al
                            for index, row in df_wordcloud.head(10).iterrows():
                                kelime = str(row[kelime_sutunu])
                                frekans = int(row[frekans_sutunu]) if frekans_sutunu else 1
                                
                                stats['wordcloud_detaylari'].append({
                                    "kelime": kelime, 
                                    "frekans": frekans
                                })
                            
                            print(f"âœ… Wordcloud detaylarÄ± yÃ¼klendi: En sÄ±k kelime '{stats['en_sik_kelime']}'")
                        
            except Exception as e:
                print(f"âŒ Wordcloud istatistikleri okunurken hata: {e}")
                # CSV formatÄ±nÄ± debug iÃ§in yazdÄ±r
                try:
                    print(f"ğŸ” CSV sÃ¼tunlarÄ±: {list(df_wordcloud.columns)}")
                    print(f"ğŸ” Ä°lk satÄ±r: {df_wordcloud.iloc[0].to_dict()}")
                except:
                    pass
        
        # Tweet sayÄ±sÄ±nÄ± fallback deÄŸerleriyle ayarla
        if stats['tweet_sayisi'] == 0:
            stats['tweet_sayisi'] = 246  # VarsayÄ±lan deÄŸer
        
        print(f"ğŸ“Š Ä°statistik Ã¶zeti: {stats['tweet_sayisi']} tweet, {stats['lda_konu_sayisi']} konu, pozitif: {stats['pozitif_oran']}")
        
        # PDF oluÅŸtur
        pdf_buffer = io.BytesIO()
        
        # Dataset adÄ±
        dataset_name = _extract_dataset_name_from_folder(analiz_klasor_yolu.name)
        
        # PDF belgesi oluÅŸtur
        doc = SimpleDocTemplate(pdf_buffer, pagesize=A4)
        styles = getSampleStyleSheet()
        
        # Ã–zel stiller
        title_style = ParagraphStyle(
            'TitleStyle',
            parent=styles['Title'],
            fontSize=20,
            spaceAfter=20,
            textColor=Color(0.2, 0.2, 0.8)
        )
        
        heading_style = ParagraphStyle(
            'HeadingStyle',
            parent=styles['Heading1'],
            fontSize=14,
            spaceAfter=12,
            textColor=Color(0.1, 0.1, 0.6)
        )
        
        content = []
        
        # BaÅŸlÄ±k
        content.append(Paragraph(f"{dataset_name} - Twitter Analiz Raporu", title_style))
        content.append(Spacer(1, 12))
        
        # Genel bilgiler
        content.append(Paragraph("Analiz Ã–zeti", heading_style))
        content.append(Paragraph(f"â€¢ Toplam Tweet SayÄ±sÄ±: {stats['tweet_sayisi']}", styles['Normal']))
        content.append(Paragraph(f"â€¢ LDA Konu SayÄ±sÄ±: {stats['lda_konu_sayisi']}", styles['Normal']))
        content.append(Paragraph(f"â€¢ Pozitif Oran: {stats['pozitif_oran']}", styles['Normal']))
        content.append(Paragraph(f"â€¢ En SÄ±k Kelime: {stats['en_sik_kelime']}", styles['Normal']))
        content.append(Spacer(1, 20))
        
        # LDA DetaylarÄ±
        if stats['lda_detaylari']:
            content.append(Paragraph("LDA Konu Analizi", heading_style))
            for konu in stats['lda_detaylari']:
                content.append(Paragraph(f"<b>{konu['konu_adi']}</b> (%{konu['yuzde']})", styles['Normal']))
                kelimeler = ", ".join(konu['anahtar_kelimeler'][:5])
                content.append(Paragraph(f"Anahtar kelimeler: {kelimeler}", styles['Normal']))
                content.append(Spacer(1, 8))
            content.append(Spacer(1, 20))
        
        # Sentiment DetaylarÄ±
        if stats['sentiment_detaylari']:
            content.append(Paragraph("Duygu Analizi", heading_style))
            sent = stats['sentiment_detaylari']
            content.append(Paragraph(f"â€¢ Pozitif: {sent['pozitif']['sayi']} tweet (%{sent['pozitif']['yuzde']})", styles['Normal']))
            content.append(Paragraph(f"â€¢ Negatif: {sent['negatif']['sayi']} tweet (%{sent['negatif']['yuzde']})", styles['Normal']))
            content.append(Paragraph(f"â€¢ NÃ¶tr: {sent['notr']['sayi']} tweet (%{sent['notr']['yuzde']})", styles['Normal']))
            content.append(Spacer(1, 20))
        
        # Kelime Analizi
        if stats['wordcloud_detaylari']:
            content.append(Paragraph("En SÄ±k KullanÄ±lan Kelimeler", heading_style))
            for i, kelime_data in enumerate(stats['wordcloud_detaylari'][:10], 1):
                content.append(Paragraph(f"{i}. {kelime_data['kelime']}: {kelime_data['frekans']} kez", styles['Normal']))
            content.append(Spacer(1, 20))
        
        # SonuÃ§
        content.append(Paragraph("Analiz SonuÃ§larÄ±", heading_style))
        content.append(Paragraph(_generate_ai_general_comment(dataset_name, analiz_info.get('params', {})), styles['Normal']))
        
        # PDF'i oluÅŸtur
        doc.build(content)
        pdf_buffer.seek(0)
        
        # Dosya adÄ±
        tarih = datetime.now().strftime('%Y%m%d_%H%M')
        dosya_adi = f"{dataset_name.replace(' ', '_')}_Twitter_Analiz_Raporu_{tarih}.pdf"
        
        return send_file(
            pdf_buffer,
            as_attachment=True,
            download_name=dosya_adi,
            mimetype='application/pdf'
        )
        
    except Exception as e:
        print(f"âŒ PDF rapor hatasÄ±: {e}")
        import traceback
        traceback.print_exc()
        return jsonify({'success': False, 'error': f'PDF rapor oluÅŸturulamadÄ±: {str(e)}'}), 500

def _extract_dataset_name_from_folder(folder_name):
    """KlasÃ¶r adÄ±ndan dataset ismini Ã§Ä±kar"""
    try:
        # Format Ã¶rnekleri:
        # MMA101Turkiye_tweetl_LDA_Duygu_Kelime_29052025_0640_d25bac99
        # AliYerlikaya_tweetle_LDA_Duygu_Kelime_29052025_0550_1fea9334
        # test_LDA_Duygu_Kelime_29052025_0548_e909d8ff
        
        print(f"ğŸ” Dataset ismi Ã§Ä±karÄ±lÄ±yor: {folder_name}")
        
        # Ã–nce '_' ile bÃ¶l
        parts = folder_name.split('_')
        
        if len(parts) >= 1:
            # Ä°lk parÃ§a dataset ismidir
            dataset_name = parts[0]
            
            print(f"ğŸ“ Bulunan dataset ismi: '{dataset_name}'")
            
            # Ã–zel durum kontrollarÄ±
            if dataset_name == 'MMA101Turkiye':
                return "MMA101TÃ¼rkiye Twitter Analizi"
            elif dataset_name == 'AliYerlikaya':
                return "Ali Yerlikaya Twitter Analizi"
            elif dataset_name == 'eczozgurozel':
                return "EczacÄ± Ã–zgÃ¼r Ã–zel Twitter Analizi"
            elif dataset_name == 'gidadedektifiTR':
                return "GÄ±da Dedektifi Twitter Analizi"
            elif dataset_name == 'test':
                return "Test Twitter Analizi"
            elif dataset_name.lower().startswith('konu'):
                return "Konu-Duygu-Kelime Analizi"
            
            # Genel formatla: Ä°sim + "Twitter Analizi"
            if len(dataset_name) > 2 and not dataset_name.isdigit():
                # CamelCase veya snake_case'i dÃ¼zelt
                if dataset_name.isupper():
                    dataset_name = dataset_name.capitalize()
                elif '_' in dataset_name:
                    dataset_name = dataset_name.replace('_', ' ').title()
                elif not dataset_name[0].isupper():
                    dataset_name = dataset_name.capitalize()
                
                return f"{dataset_name} Twitter Analizi"
            
        # Fallback: KlasÃ¶r adÄ±ndan akÄ±llÄ± Ã§Ä±karÄ±m
        if 'MMA101' in folder_name:
            return "MMA101TÃ¼rkiye Twitter Analizi"
        elif 'Ali' in folder_name or 'yerlikaya' in folder_name.lower():
            return "Ali Yerlikaya Twitter Analizi"
        elif 'ecz' in folder_name.lower() or 'ozel' in folder_name.lower():
            return "EczacÄ± Ã–zgÃ¼r Ã–zel Twitter Analizi"
        elif 'gida' in folder_name.lower():
            return "GÄ±da Dedektifi Twitter Analizi"
        elif 'test' in folder_name.lower():
            return "Test Twitter Analizi"
        elif 'konu' in folder_name.lower():
            return "Konu-Duygu-Kelime Analizi"
        
        return "Twitter Analizi"
        
    except Exception as e:
        print(f"âš ï¸ Dataset ismi Ã§Ä±karma hatasÄ±: {e}")
        return "Twitter Analizi"

def _generate_ai_general_comment(dataset_name, params):
    """AI genel yorumu oluÅŸtur"""
    analiz_turleri = params.get('analiz_turleri', [])
    konu_sayisi = params.get('lda_konu_sayisi', 5)
    
    comment = f"{dataset_name} kullanÄ±cÄ±sÄ±nÄ±n Twitter aktivitelerini analiz ettik. "
    
    if 'lda' in analiz_turleri:
        comment += f"Ä°Ã§eriklerinde {konu_sayisi} ana konu tespit edildi. "
    
    if 'sentiment' in analiz_turleri:
        comment += "Duygu analizi sonuÃ§larÄ±na gÃ¶re genel olarak dengeli bir duygu daÄŸÄ±lÄ±mÄ± gÃ¶rÃ¼lmektedir. "
    
    if 'wordcloud' in analiz_turleri:
        comment += "Kelime kullanÄ±m analizi, kullanÄ±cÄ±nÄ±n hangi konulara odaklandÄ±ÄŸÄ±nÄ± net bir ÅŸekilde ortaya koyuyor. "
    
    comment += f"Bu analiz, {dataset_name} kullanÄ±cÄ±sÄ±nÄ±n dijital ayak izini ve iÃ§erik Ã¼retim tarzÄ±nÄ± anlamamÄ±zÄ± saÄŸlÄ±yor."
    
    return comment

def _generate_ai_lda_comment(dataset_name, konu_sayisi):
    """AI LDA yorumu oluÅŸtur"""
    comments = [
        f"{dataset_name} kullanÄ±cÄ±sÄ±nÄ±n iÃ§eriklerinde {konu_sayisi} farklÄ± ana tema tespit edildi. Bu Ã§eÅŸitlilik, kullanÄ±cÄ±nÄ±n geniÅŸ bir ilgi alanÄ±na sahip olduÄŸunu gÃ¶steriyor.",
        f"Konu daÄŸÄ±lÄ±mÄ± analizi, {dataset_name} kullanÄ±cÄ±sÄ±nÄ±n en Ã§ok hangi konularda aktif olduÄŸunu ortaya koyuyor. Bu bilgi, iÃ§erik stratejisi geliÅŸtirmek iÃ§in deÄŸerli.",
        f"LDA modelimiz {konu_sayisi} konu tespit etti. Bu konular arasÄ±ndaki daÄŸÄ±lÄ±m, kullanÄ±cÄ±nÄ±n hangi alanlarda uzman olduÄŸunu gÃ¶steriyor."
    ]
    
    import random
    return random.choice(comments)

def _generate_ai_sentiment_comment(dataset_name):
    """AI duygu yorumu oluÅŸtur"""
    comments = [
        f"{dataset_name} kullanÄ±cÄ±sÄ±nÄ±n Tweet'lerinde genel olarak pozitif bir yaklaÅŸÄ±m gÃ¶ze Ã§arpÄ±yor. Bu, marka itibarÄ± aÃ§Ä±sÄ±ndan olumlu bir gÃ¶sterge.",
        f"Duygu analizi sonuÃ§larÄ±, {dataset_name} kullanÄ±cÄ±sÄ±nÄ±n dengeli ve yapÄ±cÄ± bir iletiÅŸim tarzÄ±na sahip olduÄŸunu ortaya koyuyor.",
        f"Pozitif duygu oranÄ±nÄ±n yÃ¼ksek olmasÄ±, {dataset_name} kullanÄ±cÄ±sÄ±nÄ±n topluluk Ã¼zerinde olumlu etki yarattÄ±ÄŸÄ±nÄ± gÃ¶steriyor."
    ]
    
    import random
    return random.choice(comments)

def _generate_ai_wordcloud_comment(dataset_name):
    """AI kelime bulutu yorumu oluÅŸtur"""
    comments = [
        f"{dataset_name} kullanÄ±cÄ±sÄ±nÄ±n en sÄ±k kullandÄ±ÄŸÄ± kelimeler, ilgi alanlarÄ±nÄ± ve uzmanlÄ±k konularÄ±nÄ± net bir ÅŸekilde yansÄ±tÄ±yor.",
        f"Kelime sÄ±klÄ±ÄŸÄ± analizi, {dataset_name} kullanÄ±cÄ±sÄ±nÄ±n hangi terimleri Ã¶ncelediÄŸini ve ne tÃ¼r bir dil kullandÄ±ÄŸÄ±nÄ± gÃ¶steriyor.",
        f"Kelime bulutu analizi, {dataset_name} kullanÄ±cÄ±sÄ±nÄ±n iÃ§erik stratejisinin ana pillarlarÄ±nÄ± ortaya Ã§Ä±karÄ±yor."
    ]
    
    import random
    return random.choice(comments)

def _generate_ai_conclusion(dataset_name, params):
    """AI genel sonuÃ§ yorumu oluÅŸtur"""
    return f"""
    Bu kapsamlÄ± analiz sonucunda {dataset_name} kullanÄ±cÄ±sÄ±nÄ±n Twitter kullanÄ±m profilini detaylÄ±ca inceledik. 
    
    <b>Ã–ne Ã‡Ä±kan Bulgular:</b>
    â€¢ Ä°Ã§erik Ã§eÅŸitliliÄŸi ve konu daÄŸÄ±lÄ±mÄ± dengeli
    â€¢ Duygu analizi sonuÃ§larÄ± pozitif yÃ¶nde
    â€¢ Kelime kullanÄ±mÄ± tutarlÄ± ve anlamlÄ±
    
    <b>Ã–nerilerimiz:</b>
    â€¢ Mevcut pozitif imajÄ± korumaya devam edin
    â€¢ Ä°Ã§erik Ã§eÅŸitliliÄŸini artÄ±rarak reach'i geniÅŸletin
    â€¢ Engagement oranlarÄ±nÄ± yÃ¼kseltmek iÃ§in etkileÅŸimli iÃ§erikler Ã¼retin
    
    Bu analiz bulgularÄ±nÄ± kullanarak sosyal medya stratejinizi optimize edebilir ve daha etkili bir dijital varlÄ±k oluÅŸturabilirsiniz.
    """

@analiz_bp.route('/analiz-istatistikleri/<analiz_id>', methods=['GET'])
def analiz_istatistikleri(analiz_id):
    """Belirtilen analize ait detaylÄ± istatistikleri ve sonuÃ§larÄ± dÃ¶ndÃ¼rÃ¼r."""
    try:
        if analiz_id not in aktif_analizler:
            return jsonify({"success": False, "error": "Analiz bulunamadÄ±"}), 404

        analiz_verisi = aktif_analizler[analiz_id]
        if analiz_verisi['durum'] != 'tamamlandÄ±':
            return jsonify({"success": False, "error": "Analiz henÃ¼z tamamlanmadÄ±"}), 202 # Accepted

        # Analiz klasÃ¶rÃ¼nÃ¼ geliÅŸmiÅŸ yÃ¶ntemle bul
        sonuclar_klasoru = current_app.config['SONUCLAR_FOLDER']
        analiz_klasor_yolu = None
        
        # 1. Ã–nce aktif_analizler'den gerÃ§ek klasÃ¶r adÄ±nÄ± al
        if 'klasor_adi' in analiz_verisi:
            potansiyel_klasor = sonuclar_klasoru / analiz_verisi['klasor_adi']
            if potansiyel_klasor.exists():
                analiz_klasor_yolu = potansiyel_klasor
                print(f"âœ… Ä°statistik - KlasÃ¶r aktif_analizler'den bulundu: {analiz_verisi['klasor_adi']}")
        
        # 2. DoÄŸrudan analiz_id ile klasÃ¶r ara
        if not analiz_klasor_yolu:
            potansiyel_klasor = sonuclar_klasoru / analiz_id
            if potansiyel_klasor.exists():
                analiz_klasor_yolu = potansiyel_klasor
                print(f"âœ… Ä°statistik - DoÄŸrudan klasÃ¶r bulundu: {analiz_id}")
        
        # 3. Analiz ID'nin sonunda olduÄŸu klasÃ¶rleri ara
        if not analiz_klasor_yolu:
            for klasor in sonuclar_klasoru.iterdir():
                if klasor.is_dir():
                    # KlasÃ¶r adÄ± analiz_id ile bitiyorsa
                    if klasor.name.endswith(f'_{analiz_id}'):
                        analiz_klasor_yolu = klasor
                        print(f"âœ… Ä°statistik - Son ek ile klasÃ¶r bulundu: {klasor.name}")
                        break
                    # Veya klasÃ¶r adÄ±nda analiz_id geÃ§iyorsa
                    elif analiz_id in klasor.name:
                        analiz_klasor_yolu = klasor
                        print(f"âœ… Ä°statistik - Ä°Ã§erik ile klasÃ¶r bulundu: {klasor.name}")
                        break
        
        if not analiz_klasor_yolu:
            print(f"âŒ Ä°statistik - Analiz klasÃ¶rÃ¼ bulunamadÄ±: {analiz_id}")
            return jsonify({"success": False, "error": "Analiz klasÃ¶rÃ¼ bulunamadÄ±"}), 404

        print(f"ğŸ“Š Analiz klasÃ¶rÃ¼ bulundu: {analiz_klasor_yolu}")

        stats = {
            "tweet_sayisi": 0,
            "lda_konu_sayisi": 0,
            "en_sik_kelime": "N/A",
            "pozitif_oran": "0%",
            "lda_detaylari": [],
            "sentiment_detaylari": {},
            "wordcloud_detaylari": []
        }

        # 1. Tweet SayÄ±sÄ±
        stats['tweet_sayisi'] = analiz_verisi.get('tweet_sayisi') or analiz_verisi.get('params', {}).get('tweet_sayisi', 0)
        
        # 2. LDA DetaylarÄ±
        lda_klasor = analiz_klasor_yolu / 'lda'
        if lda_klasor.exists():
            try:
                print(f"ğŸ” LDA klasÃ¶rÃ¼ kontrol ediliyor: {lda_klasor}")
                
                # Konu daÄŸÄ±lÄ±mÄ±nÄ± oku
                konu_dagilim_csv = lda_klasor / 'dokuman_konu_dagilimi.csv'
                konu_yuzdeleri = {}
                if konu_dagilim_csv.exists():
                    print(f"ğŸ“„ CSV dosyasÄ± okunuyor: {konu_dagilim_csv}")
                    df_lda_dagilim = pd.read_csv(konu_dagilim_csv)
                    if stats['tweet_sayisi'] == 0:
                        stats['tweet_sayisi'] = len(df_lda_dagilim)
                    
                    konu_sayimlari = df_lda_dagilim['dominant_konu'].value_counts()
                    toplam_dokuman = len(df_lda_dagilim)
                    
                    for konu_id, sayi in konu_sayimlari.items():
                        konu_yuzdeleri[int(konu_id)] = (sayi / toplam_dokuman) * 100
                    
                    print(f"ğŸ“Š Konu yÃ¼zdeleri hesaplandÄ±: {konu_yuzdeleri}")
                
                # DetaylÄ± konularÄ± oku
                detayli_konular_txt = lda_klasor / 'detayli_konular.txt'
                if detayli_konular_txt.exists():
                    print(f"ğŸ“ DetaylÄ± konular okunuyor: {detayli_konular_txt}")
                    with open(detayli_konular_txt, 'r', encoding='utf-8') as f:
                        content = f.read()
                        
                    # KONU X baÅŸlÄ±klarÄ±nÄ± ve kelimelerini bul
                    konu_pattern = r"KONU (\d+):\n-*\n.*?\nEn Ã¶nemli kelimeler:\n(.*?)(?:\n==================================================|\Z)"
                    konu_eslesmeler = re.findall(konu_pattern, content, re.DOTALL)
                    
                    for konu_id_str, kelime_blogu in konu_eslesmeler:
                        konu_id = int(konu_id_str)
                        anahtar_kelimeler = []
                        
                        # Kelime satÄ±rlarÄ±nÄ± parse et
                        for line in kelime_blogu.strip().split('\n'):
                            if line.strip().startswith('â€¢'):
                                # â€¢ word: 0.123 formatÄ±nÄ± parse et
                                kelime_match = re.match(r"\s*â€¢\s*([^:]+):\s*\d+\.\d+", line.strip())
                                if kelime_match:
                                    kelime = kelime_match.group(1).strip('"')
                                    anahtar_kelimeler.append(kelime)
                        
                        # Konu ismini anahtar kelimelerden tÃ¼ret
                        if anahtar_kelimeler:
                            ilk_kelimeler = anahtar_kelimeler[:2]
                            konu_adi = f"Konu {konu_id}: {' & '.join([k.title() for k in ilk_kelimeler])}"
                        else:
                            konu_adi = f"Konu {konu_id}"
                        
                        stats['lda_detaylari'].append({
                            "konu_id": konu_id,
                            "konu_adi": konu_adi,
                            "yuzde": round(konu_yuzdeleri.get(konu_id, 0), 1),
                            "anahtar_kelimeler": anahtar_kelimeler[:5]
                        })
                    
                    stats['lda_konu_sayisi'] = len(stats['lda_detaylari'])
                    print(f"âœ… LDA detaylarÄ± yÃ¼klendi: {stats['lda_konu_sayisi']} konu")
                    
                else:
                    # EÄŸer detaylÄ± konular dosyasÄ± yoksa parametrelerden al
                    stats['lda_konu_sayisi'] = analiz_verisi.get('params', {}).get('lda_konu_sayisi', 2)
                    print(f"âš ï¸ LDA detaylÄ± konular dosyasÄ± bulunamadÄ±, parametreden alÄ±nan konu sayÄ±sÄ±: {stats['lda_konu_sayisi']}")
                    
            except Exception as e:
                print(f"âŒ LDA istatistikleri okunurken hata: {e}")
                # Hata durumunda da parametrelerden al
                stats['lda_konu_sayisi'] = analiz_verisi.get('params', {}).get('lda_konu_sayisi', 2)

        # 3. Sentiment DetaylarÄ±
        sentiment_klasor = analiz_klasor_yolu / 'sentiment'
        if sentiment_klasor.exists():
            try:
                print(f"ğŸ˜Š Sentiment klasÃ¶rÃ¼ kontrol ediliyor: {sentiment_klasor}")
                
                sentiment_csv = sentiment_klasor / 'duygu_analizi_sonuclari.csv'
                if sentiment_csv.exists():
                    print(f"ğŸ“„ Sentiment CSV okunuyor: {sentiment_csv}")
                    df_sentiment = pd.read_csv(sentiment_csv)
                    toplam_tweet_sentiment = len(df_sentiment)
                    
                    if toplam_tweet_sentiment > 0:
                        # Duygu sÄ±nÄ±flarÄ±nÄ± say
                        pozitif_sayi = len(df_sentiment[df_sentiment['duygu_sinifi'].str.lower() == 'positive'])
                        negatif_sayi = len(df_sentiment[df_sentiment['duygu_sinifi'].str.lower() == 'negative'])
                        notr_sayi = len(df_sentiment[df_sentiment['duygu_sinifi'].str.lower() == 'neutral'])
                        
                        stats['sentiment_detaylari'] = {
                            "pozitif": {
                                "sayi": pozitif_sayi, 
                                "yuzde": round((pozitif_sayi / toplam_tweet_sentiment) * 100, 1)
                            },
                            "negatif": {
                                "sayi": negatif_sayi, 
                                "yuzde": round((negatif_sayi / toplam_tweet_sentiment) * 100, 1)
                            },
                            "notr": {
                                "sayi": notr_sayi, 
                                "yuzde": round((notr_sayi / toplam_tweet_sentiment) * 100, 1)
                            }
                        }
                        stats['pozitif_oran'] = f"{stats['sentiment_detaylari']['pozitif']['yuzde']}%"
                        
                        print(f"âœ… Sentiment detaylarÄ± yÃ¼klendi: P:{pozitif_sayi}, N:{negatif_sayi}, NÃ¶tr:{notr_sayi}")
                        
            except Exception as e:
                print(f"âŒ Sentiment istatistikleri okunurken hata: {e}")

        # 4. Wordcloud DetaylarÄ±
        wordcloud_klasor = analiz_klasor_yolu / 'wordcloud'
        if wordcloud_klasor.exists():
            try:
                print(f"â˜ï¸ Wordcloud klasÃ¶rÃ¼ kontrol ediliyor: {wordcloud_klasor}")
                
                en_sik_kelimeler_csv = wordcloud_klasor / 'en_sik_kelimeler.csv'
                if en_sik_kelimeler_csv.exists():
                    print(f"ğŸ“„ En sÄ±k kelimeler CSV okunuyor: {en_sik_kelimeler_csv}")
                    df_wordcloud = pd.read_csv(en_sik_kelimeler_csv)
                    
                    if not df_wordcloud.empty:
                        # SÃ¼tun isimlerini dinamik olarak bul
                        kelime_sutunu = None
                        frekans_sutunu = None
                        
                        for col in df_wordcloud.columns:
                            col_lower = col.lower()
                            if 'kelime' in col_lower or 'word' in col_lower:
                                kelime_sutunu = col
                            elif 'frekans' in col_lower or 'freq' in col_lower or 'count' in col_lower:
                                frekans_sutunu = col
                        
                        # EÄŸer sÃ¼tun isimleri bulunamazsa ilk iki sÃ¼tunu kullan
                        if not kelime_sutunu and len(df_wordcloud.columns) >= 1:
                            kelime_sutunu = df_wordcloud.columns[0]
                        if not frekans_sutunu and len(df_wordcloud.columns) >= 2:
                            frekans_sutunu = df_wordcloud.columns[1]
                        
                        if kelime_sutunu:
                            stats['en_sik_kelime'] = str(df_wordcloud.iloc[0][kelime_sutunu])
                            
                            # Ä°lk 10 kelimeyi al
                            for index, row in df_wordcloud.head(10).iterrows():
                                kelime = str(row[kelime_sutunu])
                                frekans = int(row[frekans_sutunu]) if frekans_sutunu else 1
                                
                                stats['wordcloud_detaylari'].append({
                                    "kelime": kelime, 
                                    "frekans": frekans
                                })
                            
                            print(f"âœ… Wordcloud detaylarÄ± yÃ¼klendi: En sÄ±k kelime '{stats['en_sik_kelime']}'")
                        
            except Exception as e:
                print(f"âŒ Wordcloud istatistikleri okunurken hata: {e}")
                # CSV formatÄ±nÄ± debug iÃ§in yazdÄ±r
                try:
                    print(f"ğŸ” CSV sÃ¼tunlarÄ±: {list(df_wordcloud.columns)}")
                    print(f"ğŸ” Ä°lk satÄ±r: {df_wordcloud.iloc[0].to_dict()}")
                except:
                    pass
        
        # Tweet sayÄ±sÄ±nÄ± fallback deÄŸerleriyle ayarla
        if stats['tweet_sayisi'] == 0:
            stats['tweet_sayisi'] = 246  # VarsayÄ±lan deÄŸer
        
        print(f"ğŸ“Š Ä°statistik Ã¶zeti: {stats['tweet_sayisi']} tweet, {stats['lda_konu_sayisi']} konu, pozitif: {stats['pozitif_oran']}")
        
        return jsonify({"success": True, "stats": stats})
        
    except Exception as e:
        print(f"âŒ Analiz istatistikleri hatasÄ±: {e}")
        import traceback
        traceback.print_exc()
        return jsonify({'success': False, 'error': f'Analiz istatistikleri hatasÄ±: {str(e)}'}), 500

@analiz_bp.route('/sil/<analiz_id>', methods=['DELETE'])
def analiz_sil(analiz_id):
    """Analizi siler"""
    try:
        import shutil
        import os
        
        print(f"ğŸ—‘ï¸ Analiz silme isteniyor: {analiz_id}")
        
        # Aktif analizlerden Ã§Ä±kar
        if analiz_id in aktif_analizler:
            del aktif_analizler[analiz_id]
            print(f"âœ… Analiz aktif listeden Ã§Ä±karÄ±ldÄ±: {analiz_id}")
        
        # Dosya sisteminden sil - geliÅŸmiÅŸ klasÃ¶r bulma
        sonuclar_klasoru = current_app.config['SONUCLAR_FOLDER']
        analiz_klasor_yolu = None
        
        # Analiz klasÃ¶rÃ¼nÃ¼ bul
        for klasor in sonuclar_klasoru.iterdir():
            if klasor.is_dir():
                # KlasÃ¶r adÄ± analiz_id ile bitiyorsa
                if klasor.name.endswith(f'_{analiz_id}'):
                    analiz_klasor_yolu = klasor
                    print(f"âœ… Sil - Son ek ile klasÃ¶r bulundu: {klasor.name}")
                    break
                # Veya klasÃ¶r adÄ±nda analiz_id geÃ§iyorsa
                elif analiz_id in klasor.name:
                    analiz_klasor_yolu = klasor
                    print(f"âœ… Sil - Ä°Ã§erik ile klasÃ¶r bulundu: {klasor.name}")
                    break
                # Veya doÄŸrudan analiz_id eÅŸleÅŸiyorsa
                elif klasor.name == analiz_id:
                    analiz_klasor_yolu = klasor
                    print(f"âœ… Sil - Tam eÅŸleÅŸme ile klasÃ¶r bulundu: {klasor.name}")
                    break
        
        if analiz_klasor_yolu and analiz_klasor_yolu.exists():
            # KlasÃ¶rÃ¼ tamamen sil
            shutil.rmtree(analiz_klasor_yolu)
            print(f"âœ… Analiz klasÃ¶rÃ¼ silindi: {analiz_klasor_yolu}")
        else:
            print(f"âš ï¸ Silinecek klasÃ¶r bulunamadÄ±: {analiz_id}")
        
        return jsonify({
            'success': True,
            'message': 'Analiz baÅŸarÄ±yla silindi'
        })
        
    except Exception as e:
        print(f"âŒ Analiz silme hatasÄ±: {e}")
        return jsonify({
            'success': False,
            'error': f'Analiz silinirken hata oluÅŸtu: {str(e)}'
        }), 500